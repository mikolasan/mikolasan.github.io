{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/markov-decision-processes","result":{"data":{"markdownRemark":{"html":"<p>Neural networks, classification and clusterization algorithms are considered to be intelligent as far as they are part of Artificial Intelligence. But can they create novel independent decisions? Plan their execution algorithm and improve it over time?</p>\n<p>And when I ask these questions I don't want to focus on specific aspects like \"improving the algorithm\". Because I can manually add some genetic algorithm to the system, and voila, it improves some characteristic over time. Instead I want the system to decide itself if it needs improvement, or a change - all of it depending on its goals.</p>\n<p>In another words, if the environment changes, the system can use its prior knowledge to learn and adapt to new conditions. In terms of current AI paradigm: it doesn't need new training and testing set and training process for many hours and days on many GPUs. Is it possible?</p>\n<p>Appropriate keywords:</p>\n<ul>\n<li>incremental learning</li>\n<li>generative neural networks</li>\n</ul>\n<h2 id=\"papers\" style=\"position:relative;\">Papers<a href=\"#papers\" aria-label=\"papers permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"http://incompleteideas.net/book/the-book.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Reinforcement Learning</a>: An Introduction by Richard S. Sutton and Andrew G. Barto  <a href=\"https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Another link to PDF</a></li>\n<li>MDP formal description <a href=\"https://arxiv.org/pdf/2006.05879.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://arxiv.org/pdf/2006.05879.pdf</a> (wikipedia as always sucks)</li>\n<li>More math <a href=\"https://games-automata-play.github.io/blog/dynamic_algorithms_MDP/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://games-automata-play.github.io/blog/dynamic_algorithms_MDP/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1105.5460.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Big review of many other planning techniques</a> in <strong>Decision-Theoretic Planning: Structural assumptions and Computational Leverage</strong>. Task of making coffee, checking mail and cleaning the room. MDP to bayesian network. Task decomposition - partitioning of the state space into blocks - exactly my thought that infinite grid world doesn't exist, it's only possible to process small rooms and then combine them. But how such rooms are created?</li>\n<li><a href=\"/ai/reviews/reward-is-not-necessary\">Reward is not Necessary</a>. Temporal goals <a href=\"https://arxiv.org/pdf/2211.10851.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://arxiv.org/pdf/2211.10851.pdf</a></li>\n<li>Unpredictable transitions <a href=\"https://gaips.inesc-id.pt/~fmelo/publications/witwicki13icaps.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://gaips.inesc-id.pt/~fmelo/publications/witwicki13icaps.pdf</a></li>\n<li>Dynamic states with fixed initial one <a href=\"https://finale.seas.harvard.edu/files/finale/files/doshi-velez-tpami-2015.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://finale.seas.harvard.edu/files/finale/files/doshi-velez-tpami-2015.pdf</a></li>\n<li>Dynamic state space <a href=\"https://www.frontiersin.org/articles/10.3389/fncom.2021.784592/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.frontiersin.org/articles/10.3389/fncom.2021.784592/full</a> &#x3C;&#x3C;&#x3C;=== this is something very related to what I was - hoping to find. It's 2022, imagine how fresh it is!</li>\n<li>When there is a reward function and values assigned to the states, some of them receive significant values which makes them standout across the others. Such special states we call goals - because they apparently very important for the agent. With some logic added to this we can decompose goals and have more meaningfull model. <strong>Prioritized Goal Decomposition of Markov Decision Processes: Toward a Synthesis of Classical and Decision Theoretic Planning</strong> <a href=\"https://www.cs.toronto.edu/~cebly/Papers/decomposition.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">PDF</a></li>\n<li><a href=\"https://openreview.net/pdf?id=V6BjBgku7Ro\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">GLAMOR</a> - learn the world models by modeling inverse dynamics. <a href=\"https://github.com/keirp/glamor\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Code</a>. Use conditional probabilities and Bayes' rule to find the best sequence of actions (like language sequences of tokens)</li>\n</ul>\n<h2 id=\"code\" style=\"position:relative;\">Code<a href=\"#code\" aria-label=\"code permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>May be good for the implementation</p>\n<ul>\n<li>Straight-forward implementation, but code can be useful <a href=\"https://github.com/sawcordwell/pymdptoolbox/blob/master/src/mdptoolbox/mdp.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/sawcordwell/pymdptoolbox/blob/master/src/mdptoolbox/mdp.py</a></li>\n<li>Not bad Python code <a href=\"https://github.com/coverdrive/MDP-DP-RL/blob/master/src/processes/mdp.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/coverdrive/MDP-DP-RL/blob/master/src/processes/mdp.py</a></li>\n<li><a href=\"https://github.com/infer-actively/pymdp\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/infer-actively/pymdp</a></li>\n</ul>\n<p>My implementation of MDP didn't converge. In reinforcement learning the reward function must be designed for the task, and I put there wrong values. I put 10 points for reaching the goal, -20 as penalty for other states. Changing to 0 and -1 respectively gives 100% convergence. Here's the <a href=\"https://link.springer.com/article/10.1023/A:1007355226281\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">source of enlightenment</a></p>\n<blockquote>\n<p>When the problem solver reaches a goal state, we can provide a fixed reward (e.g., zero) and terminate the search (i.e., the goal states are absorbing states). With this reward function, the cumulative reward of a policy is equal to the negative of the cost of solving the problem using that policy. Hence, the optimal policy will be the policy that solves the problem most efficiently.</p>\n</blockquote>\n<h3 id=\"examples\" style=\"position:relative;\">Examples<a href=\"#examples\" aria-label=\"examples permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<ul>\n<li><a href=\"/ai/mdp-example-social-media-bot\">Social media bot</a></li>\n</ul>\n<h2 id=\"value-iteration-networks\" style=\"position:relative;\">Value Iteration Networks<a href=\"#value-iteration-networks\" aria-label=\"value iteration networks permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>There is this repo <a href=\"https://github.com/itdxer/neupy\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">here</a></p>\n<blockquote>\n<p>Exploring world with Value Iteration Network (VIN) One of the basic applications of the Value Iteration Network that learns how to find an optimal path between two points in the environment with obstacles.</p>\n</blockquote>\n<p>from an article (2017 Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel - <a href=\"https://arxiv.org/pdf/1602.02867.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Value Iteration Networks</a>)</p>\n<blockquote>\n<p>We introduce the value iteration network (VIN): a fully differentiable neural network with a ‘planning module’ embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning.</p>\n</blockquote>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"flex:1.7717391304347827;\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c28a3ef59c90d7edd6507693664c0c28/0f98f/vin-figure-1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.44171779141104%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIEA//EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAb0sQ0AP/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAECEBETMf/aAAgBAQABBQKaSF3VG8s//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAHRAAAQIHAAAAAAAAAAAAAAAAAQAxAhEgITJBkf/aAAgBAQAGPwIZKGxeTrfTR//EABwQAQABBAMAAAAAAAAAAAAAAAEAESFBURAxYf/aAAgBAQABPyFXZkVgaFlrX9b4NiCz2n//2gAMAwEAAgADAAAAEDDP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHRABAAMAAgMBAAAAAAAAAAAAAQARITHwQVGhwf/aAAgBAQABPxAtFlxpz4suPvFpBrXtnH2PVfsQLmYTlXozsJ//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"VIN figure 1\"\n        title=\"Typical example of use\"\n        src=\"/static/c28a3ef59c90d7edd6507693664c0c28/6aca1/vin-figure-1.jpg\"\n        srcset=\"/static/c28a3ef59c90d7edd6507693664c0c28/d2f63/vin-figure-1.jpg 163w,\n/static/c28a3ef59c90d7edd6507693664c0c28/c989d/vin-figure-1.jpg 325w,\n/static/c28a3ef59c90d7edd6507693664c0c28/6aca1/vin-figure-1.jpg 650w,\n/static/c28a3ef59c90d7edd6507693664c0c28/7c09c/vin-figure-1.jpg 975w,\n/static/c28a3ef59c90d7edd6507693664c0c28/01ab0/vin-figure-1.jpg 1300w,\n/static/c28a3ef59c90d7edd6507693664c0c28/0f98f/vin-figure-1.jpg 1920w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Typical example of use</p></figcaption>\n  </figure></p>\n<p>Here's a bold claim accusing RL in only maximizing one action over a smart strategy</p>\n<blockquote>\n<p>[Typical NNs] are inherently reactive, and in particular, lack explicit planning computation. The success of reactive policies in sequential problems is due to the learning algorithm, which essentially trains a reactive policy to select actions that have good long-term consequences in its training domain.</p>\n</blockquote>\n<p>So I skipped simple ANN implementations and <a href=\"https://github.com/avivt/VIN\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">tried VIN</a>. I had git problems here. I found a fork with a fix. I also found that VIN is implemented in <a href=\"https://github.com/kentsommer/pytorch-value-iteration-networks\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">PyTorch</a> and <a href=\"https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">TensorFlow</a> which means I don't need to use neupy library.</p>\n<h2 id=\"questions\" style=\"position:relative;\">Questions<a href=\"#questions\" aria-label=\"questions permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>From the MDP definition I do not see if an agent can carry information obtained in one of the states, in case if we view a state as a position in the two-dimensional grid.</p>\n<p>Because if we add that information to the state, then all states must include this extra level, so in another terms, the agent travels into another world, where the grid is the same, but it must learn transitions between them from scratch.</p>\n<p>Here's an illustration of two connected worlds I found in the article: one without mail in the mailbox (M = false) and another when a postman delivered mail (M = true). Otherwise the loop is the same.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; flex:1.7717391304347827;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f16341b86a72535681022430c263980d/75609/mdp-transition-graph.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.44171779141104%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABQ0lEQVR42qWS23KDMAxE8/+/2CSdwIDN3RcwNsG0B9yGtq/VAyNb2tWuzOXjH3F5ZfM8T9PkvY8xcnw+n8uykBhjyOO6hhCcc33fb9t2gtNBqaEo9wAD3lqjD5gxGprJuWHoi6IQQjDmBPsQhJTW2i3GcRyllADcPLsjlhAUxEqFZQneU+26LuF3cNs2VVUx5FARmSzK8vHIkIooWtu2TVVMvV2vWZZxs4NRKKWAmzJjsUSitc7zPByem6ZJPsfRIoR+xtL5Ldv7JAMYOd+6rpXWk5vAIIQjuzRar/iyVgg5DMMXmH3CDSWtWIUpz7Lb7aaUZgJ96GIjNKzrys39/k7p3Db04GHBTDKJQgRDRbmqa5bC8HGcUEQbAn+9M+Wu7YQoWX5iXI+FkZdif8K+62GE4vW6JzidMfz6B37GdsSfP+wTwf18w1sNz6gAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Illustration from Decision-Theoretic Planning\"\n        title=\"\"\n        src=\"/static/f16341b86a72535681022430c263980d/a6d36/mdp-transition-graph.png\"\n        srcset=\"/static/f16341b86a72535681022430c263980d/222b7/mdp-transition-graph.png 163w,\n/static/f16341b86a72535681022430c263980d/ff46a/mdp-transition-graph.png 325w,\n/static/f16341b86a72535681022430c263980d/a6d36/mdp-transition-graph.png 650w,\n/static/f16341b86a72535681022430c263980d/e548f/mdp-transition-graph.png 975w,\n/static/f16341b86a72535681022430c263980d/75609/mdp-transition-graph.png 994w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>","excerpt":"Neural networks, classification and clusterization algorithms are considered to be intelligent as far as they are part of Artificial…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#papers\">Papers</a></p>\n</li>\n<li>\n<p><a href=\"#code\">Code</a></p>\n<ul>\n<li><a href=\"#examples\">Examples</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#value-iteration-networks\">Value Iteration Networks</a></p>\n</li>\n<li>\n<p><a href=\"#questions\">Questions</a></p>\n</li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/bc7cf9ce-5862-558c-8c92-8a90d57c1b3b.jpg"},"frontmatter":{"date":"January 01, 2023","published":"January 01, 2023","lastModified":"January 01, 2023","title":"Markov decision processes","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/markov-decision-processes.md","url":"/ai/markov-decision-processes","next":{"excerpt":"I was trying to understand the \"inverse-dynamics model\" block in the paper by Mitsuo Kawato. First, I thought that somehow he's applying…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/inverse-dynamics-model.md","frontmatter":{"title":"Inverse Dynamics Model","date":"2023-03-20T00:00:00.000Z","topic":null,"article":"main"},"id":"e93d8bf0-99f9-56d0-8551-789675972af1"},"previous":{"excerpt":"Improvements First let's review improvements for Gradient Descent. They are still GD by its nature. Momentum gradient descent (MGD) Nesterov…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/alternatives-to-gradient-descent.md","frontmatter":{"title":"Alternatives to gradient descent","date":"2022-11-13T00:00:00.000Z","topic":null,"article":"main"},"id":"a39e9857-46a4-50e0-90aa-a814c61ddb0c"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}