{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/convolutional-neural-networks","result":{"data":{"markdownRemark":{"html":"<p>A long time ago, I was thinking about systems that perceive only video input. But the more I read about image processing, the more I get a notion that sole video is not enough.</p>\n<p>The system must be able to change its view position according to its internal directive. It also must move in the world and have sensor feedback about its movements. This will add so much more context that no manual object segmentation will be required.</p>\n<p>For more information on why more sensor data help to understand the world better, refer to <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405253/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Embodied Cognition</a>.</p>\n<p>However, if you want to focus on a visual processing system, then I would recommend looking at <a href=\"https://github.com/AlexeyAB/darknet\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">YOLO v4</a>.</p>\n<p>It’s written in C. This model made a breakthrough in real-time object detection in 2012.</p>\n<p>Do you know what I like about this model? It's easily compiled on Windows, and highly optimized for different GPUs. But I like it because it implements many biological principles one can find in Hubel’s book <a href=\"https://clinic.medlink.org/wp-content/uploads/2019/09/Eye_Brain_and_Vision.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">“Eye, Brain, and Vision”</a>.</p>\n<h2 id=\"main-questions\" style=\"position:relative;\">Main questions<a href=\"#main-questions\" aria-label=\"main questions permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<h3 id=\"image-size\" style=\"position:relative;\">Image size<a href=\"#image-size\" aria-label=\"image size permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>What will be the spatial map between an image with resolution of 1024x1024 pixels and rods and cells in the human eye assuming that the image covers full field of view? In such model one pixel will correspond to an area of rods and cones. In order to keep the system small and efficient, we will keep only one rod and cones according to its distribution in retina. How many photo receptors do we need for such model?</p>\n<h3 id=\"halftone-images\" style=\"position:relative;\">Halftone images<a href=\"#halftone-images\" aria-label=\"halftone images permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>Human eyes not exactly percieve the world as a matrix of RGB pixels. There are several types of <a href=\"https://en.wikipedia.org/wiki/Retinal_ganglion_cell\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">retinal ganglion cells</a>. What if we convert images using <a href=\"https://en.wikipedia.org/wiki/Halftone\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">halftone</a> technique (<a href=\"https://github.com/GravO8/halftone\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">python 1</a>, <a href=\"https://github.com/philgyford/python-halftone\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">python 2</a>, <a href=\"https://stackoverflow.com/questions/1487517/fastest-dithering-halftoning-library-in-c\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">c opencv</a>). Also: <a href=\"https://www.frontiersin.org/articles/10.3389/fncel.2021.662329/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Rod and Cone Connections With Bipolar Cells in the Rabbit Retina</a></p>\n<h3 id=\"how-does-a-convolution-kernel-get-trained\" style=\"position:relative;\">How does a convolution kernel get trained?<a href=\"#how-does-a-convolution-kernel-get-trained\" aria-label=\"how does a convolution kernel get trained permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>2D convolution is <a href=\"https://stackoverflow.com/questions/16798888/2-d-convolution-as-a-matrix-matrix-multiplication\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">a matrix-matrix multiplication</a>. See <a href=\"https://ai.stackexchange.com/questions/11172/how-can-the-convolution-operation-be-implemented-as-a-matrix-multiplication\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">here</a> with pictures and formulas. And see <a href=\"https://mydeeplearningnb.wordpress.com/2019/07/24/visualizing-cnns-using-tensorflow/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">visualization</a> of active neurons and conv filters.</p>\n<h2 id=\"papers\" style=\"position:relative;\">Papers<a href=\"#papers\" aria-label=\"papers permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>A good list compiled in <a href=\"https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this post</a> on Towards Science</p>\n<ul>\n<li>Network in Network <a href=\"https://arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>A guide to convolution arithmetic for deep learning <a href=\"https://arxiv.org/abs/1603.07285\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Deconvolution and Checkerboard Artifacts <a href=\"https://distill.pub/2016/deconv-checkerboard/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Multi-Scale Context Aggregation by Dilated Convolutions <a href=\"https://arxiv.org/abs/1511.07122\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>ResNeXt: Aggregated Residual Transformations for Deep Neural Networks <a href=\"https://arxiv.org/abs/1611.05431\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Going deeper with convolutions <a href=\"https://arxiv.org/abs/1409.4842\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Flattened convolutional neural networks for feedforward acceleration <a href=\"https://arxiv.org/abs/1412.5474\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs <a href=\"https://arxiv.org/abs/1412.7062\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Xception: Deep Learning with Depthwise Separable Convolutions <a href=\"https://arxiv.org/abs/1610.02357\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>Rethinking the Inception Architecture for Computer Vision <a href=\"https://arxiv.org/pdf/1512.00567v3.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications <a href=\"https://arxiv.org/abs/1704.04861\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n<li>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices <a href=\"https://arxiv.org/abs/1707.01083\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Link</a></li>\n</ul>\n<h3 id=\"blogs\" style=\"position:relative;\">Blogs<a href=\"#blogs\" aria-label=\"blogs permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<ul>\n<li><a href=\"https://grzegorzgwardys.wordpress.com/2016/04/22/8/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Convolutional Neural Networks backpropagation: from intuition to derivation</a> by grzegorz gwardys</li>\n<li><a href=\"https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Backpropagation In Convolutional Neural Networks</a> by Jefkine</li>\n</ul>","excerpt":"A long time ago, I was thinking about systems that perceive only video input. But the more I read about image processing, the more I get a…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#main-questions\">Main questions</a></p>\n<ul>\n<li><a href=\"#image-size\">Image size</a></li>\n<li><a href=\"#halftone-images\">Halftone images</a></li>\n<li><a href=\"#how-does-a-convolution-kernel-get-trained\">How does a convolution kernel get trained?</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#papers\">Papers</a></p>\n<ul>\n<li><a href=\"#blogs\">Blogs</a></li>\n</ul>\n</li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/93583a77-b3f1-56e5-8bb4-f637131b3ded.jpg"},"frontmatter":{"date":"May 13, 2023","published":"May 13, 2023","lastModified":"May 13, 2023","title":"Convolutional Neural Networks","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/convolutional-neural-networks.md","url":"/ai/convolutional-neural-networks","next":{"excerpt":"First thing you may find is Whisper by OpenAI. Surprisingly it's open sourced, but the normal model requires 10GB of video memory which is…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/voice-recognition-ai-model.md","frontmatter":{"title":"Voice recognition AI model","date":"2023-05-18T00:00:00.000Z","topic":null,"article":"quest"},"id":"5f4c1325-32e2-5290-b478-74980b5d1c78"},"previous":{"excerpt":"The Cart Pole problem was introduced by A.G. Barto, R.S. Sutton, and C.W. Anderson in \"Neuronlike adaptive elements that\ncan solve difficult…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/solve-cartpole-with-spiking-neural-networks.md","frontmatter":{"title":"Solving CartPole with Spiking Neural Networks","date":"2023-04-21T00:00:00.000Z","topic":null,"article":"main"},"id":"1f71d5a9-25f3-5d06-929b-2a2d00eb032a"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}