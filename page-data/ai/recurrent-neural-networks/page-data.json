{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/recurrent-neural-networks","result":{"data":{"markdownRemark":{"html":"<p>ANN should have notion of time. RNN can do it</p>\n<h2 id=\"event-sequence-prediction\" style=\"position:relative;\">Event sequence prediction<a href=\"#event-sequence-prediction\" aria-label=\"event sequence prediction permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>How to encode time in ANN? With the help of another ANN that will nonlinearly transform idle delay time into encoded input. Almost like one-hot encoding, but they call it <em>soft</em>.</p>\n<ul>\n<li><a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Recurrent Neural Networks cheatsheet</a> by Afshine Amidi and Shervine Amidi</li>\n<li><a href=\"https://towardsdatascience.com/how-to-encode-time-property-in-recurrent-neutral-networks-friday-experiment-c14c39ba9755\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">How-to encode time property in recurrent neural networks</a> (<a href=\"https://github.com/crazyleg/time-dependant-rnn-embeddings-keras\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a>, <a href=\"https://arxiv.org/pdf/1708.00065.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">article</a>)</li>\n</ul>\n<h2 id=\"exercise\" style=\"position:relative;\">Exercise<a href=\"#exercise\" aria-label=\"exercise permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>How to represent time in hidden neurons. Let’s assume that we have neurons “second”, “minute”, “hour”, “day”. How to start counting in ANN? How to write a calculator with ANN?</p>","excerpt":"ANN should have notion of time. RNN can do it Event sequence prediction How to encode time in ANN? With the help of another ANN that will…","tableOfContents":"<ul>\n<li><a href=\"#event-sequence-prediction\">Event sequence prediction</a></li>\n<li><a href=\"#exercise\">Exercise</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/ad5048df-ca32-5b3c-a369-5ac2b6e07cf4.jpg"},"frontmatter":{"date":null,"published":"January 19, 2023","lastModified":"April 29, 2023","title":"Recurrent Neural Networks","subtitle":null,"section":"brain","draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/recurrent-neural-networks.md","url":"/ai/recurrent-neural-networks","next":{"excerpt":"Roadmap Tabular (Lecture 20, Huggingface course, develop the thing from scratch) DQN (DQN Paper and implement something better than any…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reinforcement-learning.md","frontmatter":{"title":"Reinforcement Learning","date":null,"topic":true,"article":null},"id":"df72c525-91b8-57c1-8dcb-a086f877855f"},"previous":{"excerpt":"TODO ;)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reservoir-computing.md","frontmatter":{"title":"Reservoir Computing","date":null,"topic":true,"article":null},"id":"67d748a6-0bd6-5d7d-97f4-a8488525b883"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}