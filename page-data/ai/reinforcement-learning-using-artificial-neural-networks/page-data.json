{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/reinforcement-learning-using-artificial-neural-networks","result":{"data":{"markdownRemark":{"html":"<p>In this post we collect known research about Artificial Neural Networks and their property to approximate non-linear functions and logical statements. We review simple examples, and in the end we implement Reinforcement Learning purely as a Neural Network.</p>\n<blockquote>\n<p>ANN are universal function approximators</p>\n<p>Julien Pascal <a href=\"https://julienpascal.github.io/post/ann_1/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">mentioning the theorem and writing code in Julia</a></p>\n</blockquote>\n<p>This is our first step that carries important hypotesis that <strong>any algorithm can be approximated by Neural Networks</strong>.</p>\n<p>But before we start, one word from our guest star, Mathuccino.</p>\n<blockquote>\n<p>My advice is <strong>don't focus too much on articles</strong></p>\n<p><em>@mathuccino</em></p>\n</blockquote>\n<p>So practice, practice, practice! We focus on examples implemented with PyTorch and also very transparent version without any ML framework.</p>\n<h2 id=\"backpropagation-formally-introduced\" style=\"position:relative;\">Backpropagation formally introduced<a href=\"#backpropagation-formally-introduced\" aria-label=\"backpropagation formally introduced permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>very clear work with partial derivatives in <strong>Neural Networks for RF and Microwave Design</strong> by Q. J. Zhang, K. C. Gupta</li>\n<li>classic and also biologically reasonable in <a href=\"https://www.sciencedirect.com/science/article/pii/S0896627320307054\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Artificial Neural Networks for Neuroscientists</a></li>\n<li>superscript indecies for layers in <a href=\"https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this big SO question</a> based on <a href=\"http://neuralnetworksanddeeplearning.com/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">free online book</a> by Michael Neilsen</li>\n<li>sequence of matrix-vector tuples in <a href=\"https://arxiv.org/pdf/2210.00805.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Limitations of neural network training due to numerical instability of backpropagation</a></li>\n<li>multivariate vector-valued function <a href=\"https://arxiv.org/pdf/2107.09384.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">An induction proof of the backpropagation algorithm in matrix notation</a></li>\n<li>function on matrices <a href=\"https://arxiv.org/pdf/2110.06488.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex</a></li>\n<li>tensors, preactivation, activation finctions and Hadamard product in <a href=\"https://arxiv.org/pdf/2004.04729.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">DITHERED BACKPROP: A SPARSE AND QUANTIZED BACKPROPAGATION ALGORITHM FOR MORE EFFICIENT DEEP NEURAL NETWORK TRAINING</a></li>\n<li>neuron is a tree in <a href=\"https://arxiv.org/pdf/2202.02248.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Backpropagation Neural Tree</a></li>\n<li>composition of functions in <a href=\"https://arxiv.org/pdf/1502.04434.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">INVARIANT BACKPROPAGATION: HOW TO TRAIN A TRANSFORMATION-INVARIANT NEURAL NETWORK</a></li>\n<li>Moore-Penrose pseudoinverse of a matrix in <a href=\"https://arxiv.org/pdf/2011.08895.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks</a></li>\n<li>integrate-and-fire neurons with integration variable and threshold in <a href=\"https://arxiv.org/pdf/1906.00851.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes</a></li>\n</ul>\n<h2 id=\"the-simpliest-ann\" style=\"position:relative;\">The simpliest ANN<a href=\"#the-simpliest-ann\" aria-label=\"the simpliest ann permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>First of all lets create the simpliest but non trivial perceptron - <a href=\"https://medium.com/mlearning-ai/learning-xor-with-pytorch-c1c11d67ba8e\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">neuron system that works as a XOR function</a>.\nTwo input neurons, two in the hidden layer, and one neuron - output. Simple as that:</p>\n<h3 id=\"pytorch\" style=\"position:relative;\">Pytorch<a href=\"#pytorch\" aria-label=\"pytorch permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">nn.Module</a></li>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">MSELoss</a></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n\nepochs <span class=\"token operator\">=</span> <span class=\"token number\">2001</span>\nX <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nY <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">XOR</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_dim <span class=\"token operator\">=</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>XOR<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>linear1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>input_dim<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>linear2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> output_dim<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>linear1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sigmoid<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>linear2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># initialize the weight tensor and bias</span>\n    linear_layers <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>m <span class=\"token keyword\">for</span> m <span class=\"token keyword\">in</span> model<span class=\"token punctuation\">.</span>modules<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> m <span class=\"token keyword\">in</span> linear_layers<span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># here we use a normal distribution</span>\n        m<span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>normal_<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n    mseloss <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># mean squared error</span>\n    optimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.02</span><span class=\"token punctuation\">,</span> momentum<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>epochs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        y_hat <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>forward<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\n        loss <span class=\"token operator\">=</span> mseloss<span class=\"token punctuation\">(</span>y_hat<span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">)</span>\n        loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<p><a href=\"/assets/c253dd9082799a4c8c060ef2b4e4938d/ann_xor_function_pytorch.py\">Full code</a> (plotting function is skipped for clarity)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 642px; flex:1.1726618705035972;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/862cba9a238dcd64ab43d8a7bbda93ef/1bba8/pytorch-ann-xor-function.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.2760736196319%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB30lEQVR42o1T32/TMBDu3wtsq8YLQuIZCfHXbNKQxsPoeGJSn1klXqARLWRNnDj+dT7b+5K0abpOgpNlf4793X13vkw+fnj/7u2b6cnLs1cv/mfg5vn09PX09PzsZGKtjZ0lWD//y/rLoiwnRARE5KzRzrMDcrYbzxjOPG5aC0pd1y0ZnqSUXhah+B1Ih8AhphBjODTsybMhBsV7L2U9wYKNqKpILv39k5xMpkyuSYGPBTsfhPExMPXkXrbWOvDudgyJdNJl68Wbdrs76Bff2TYycijL8qhmkE7J1kkXrRymIf6ejAJhb4yptSMelX3wAkA2GdFqIXNAZvZgIjIq8fNBVZqUY1Rnz9yq7UCVp+UPvElLrkc5c0dBkZXzQjtpvecjFwCI5mlLBgJTCBFHHQJoPJfaCU2o8Kh14pOc28hKKbzk8dsgNiTABZIafByQmRlN8oQMzZa8Ug0aCjRFLLRHOSDqgIy+g2wA8LkzAK3kJl9/v79fZhl5it1RY1zR2EYbHhcMMzLP8xyOis0GuELjFw9K69VqhY84gjrVSIQxQ2+j2ePO+j7vExn+nmEeW0uuqsnlxcVsNpvP519vbz9dXX25ufl8fX139229Xi+Xv7IMqrNu3g98h5zFYvEI7PG73kk4ZUUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Output\"\n        title=\"\"\n        src=\"/static/862cba9a238dcd64ab43d8a7bbda93ef/1bba8/pytorch-ann-xor-function.png\"\n        srcset=\"/static/862cba9a238dcd64ab43d8a7bbda93ef/222b7/pytorch-ann-xor-function.png 163w,\n/static/862cba9a238dcd64ab43d8a7bbda93ef/ff46a/pytorch-ann-xor-function.png 325w,\n/static/862cba9a238dcd64ab43d8a7bbda93ef/1bba8/pytorch-ann-xor-function.png 642w\"\n        sizes=\"(max-width: 642px) 100vw, 642px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>I have a problem with 2000 epochs on some seed values. <a href=\"https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">This</a> might give an answer.</p>\n<h3 id=\"keras\" style=\"position:relative;\">Keras<a href=\"#keras\" aria-label=\"keras permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n\nX <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nY <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n\nmodel <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'sigmoid'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'binary_crossentropy'</span><span class=\"token punctuation\">,</span> metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nhistory <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3 id=\"tensorflow-no-keras\" style=\"position:relative;\">Tensorflow (no Keras)<a href=\"#tensorflow-no-keras\" aria-label=\"tensorflow no keras permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>Start with the <a href=\"https://www.tensorflow.org/guide/core/mlp_core\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">official docs</a></p>\n<h3 id=\"no-frameworks\" style=\"position:relative;\">No frameworks<a href=\"#no-frameworks\" aria-label=\"no frameworks permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>So if you think library for machine learning is not making this example any simpler, then look how it would look with no external libraries (only numpy for matrix operations <a href=\"https://numpy.org/devdocs/user/absolute_beginners.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">don't know numpy - help!</a>)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">sigmoid</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token number\">1</span> <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">+</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span> <span class=\"token operator\">*</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">relu</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>\n\nepochs <span class=\"token operator\">=</span> <span class=\"token number\">3000</span>\nlearning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.1</span>\ndata_inputs <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\ndata_outputs <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nn_neurons_per_layer <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>data_inputs<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n\nweights_1 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>n_neurons_per_layer<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> n_neurons_per_layer<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">0.1</span>\nweights_2 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>n_neurons_per_layer<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> n_neurons_per_layer<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">0.1</span>\nbias_1 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_neurons_per_layer<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nbias_2 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_neurons_per_layer<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>epochs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    n_training_steps <span class=\"token operator\">=</span> data_inputs<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n_training_steps<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> data_inputs<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n        y <span class=\"token operator\">=</span> data_outputs<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">]</span> <span class=\"token comment\"># scalar</span>\n        \n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>newaxis<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>T <span class=\"token comment\"># column vector</span>\n        <span class=\"token comment\"># first hidden layer</span>\n        y_1 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> weights_1<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias_1 <span class=\"token comment\"># why x is first? (bias - column vector)</span>\n        y_1 <span class=\"token operator\">=</span> sigmoid<span class=\"token punctuation\">(</span>y_1<span class=\"token punctuation\">)</span> <span class=\"token comment\"># column vector</span>\n        <span class=\"token comment\"># output</span>\n        y_2 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>y_1<span class=\"token punctuation\">,</span> weights_2<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias_2 <span class=\"token comment\"># column vector (just one row)</span>\n        <span class=\"token comment\"># y_2 = sigmoid(y_2)</span>\n        y_2 <span class=\"token operator\">=</span> relu<span class=\"token punctuation\">(</span>y_2<span class=\"token punctuation\">)</span>\n        \n        diff <span class=\"token operator\">=</span> y <span class=\"token operator\">-</span> y_2 <span class=\"token comment\"># 1 element matrix (column vector)</span>\n        \n        grad_2 <span class=\"token operator\">=</span> <span class=\"token number\">1</span> <span class=\"token comment\"># y_2 * (1 - y_2) # sigmoid derivative</span>\n        d_2 <span class=\"token operator\">=</span> diff <span class=\"token operator\">*</span> grad_2 <span class=\"token comment\"># 1 element matrix (column vector)</span>\n        <span class=\"token comment\"># np.sum(dZ2,axis=1,keepdims=True)</span>\n        dw2 <span class=\"token operator\">=</span> y_1<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>d_2<span class=\"token punctuation\">)</span>\n        weights_2 <span class=\"token operator\">+=</span> learning_rate <span class=\"token operator\">*</span> dw2\n        bias_2 <span class=\"token operator\">+=</span> learning_rate <span class=\"token operator\">*</span> d_2\n        \n        grad_1 <span class=\"token operator\">=</span> y_1 <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> y_1<span class=\"token punctuation\">)</span> <span class=\"token comment\"># sigmoid derivative</span>\n        d_1 <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>weights_2<span class=\"token punctuation\">.</span>T <span class=\"token operator\">*</span> d_2<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> grad_1\n        dw1 <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>d_1<span class=\"token punctuation\">)</span>\n        weights_1 <span class=\"token operator\">+=</span> learning_rate <span class=\"token operator\">*</span> dw1\n        bias_1 <span class=\"token operator\">+=</span> learning_rate <span class=\"token operator\">*</span> d_1\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">network_forward</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">global</span> weights_1<span class=\"token punctuation\">,</span> weights_2<span class=\"token punctuation\">,</span> bias_1<span class=\"token punctuation\">,</span> bias_2\n    y_1 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> weights_1<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias_1\n    y_1 <span class=\"token operator\">=</span> sigmoid<span class=\"token punctuation\">(</span>y_1<span class=\"token punctuation\">)</span>\n    y_2 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>y_1<span class=\"token punctuation\">,</span> weights_2<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias_2\n    y_2 <span class=\"token operator\">=</span> relu<span class=\"token punctuation\">(</span>y_2<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>y_2<span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>ndarray<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        y_2 <span class=\"token operator\">=</span> y_2<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> y_2\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>network_forward<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>network_forward<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>network_forward<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>network_forward<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Later you might be tempted to review such simple systems further</p>\n<ul>\n<li><a href=\"https://github.com/gokadin/ai-simplest-network\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">The simplest artificial neural network possible</a> explained and demonstrated. Even if you don't code in Go, check out the theory - it's the \"bone structure\" you need to understand. <a href=\"https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Better matrix pictures</a></li>\n<li>But if in the first place you want <strong>math stuff</strong>, then here is a <a href=\"https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">simple explanation</a>, or <a href=\"https://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">about gradients</a>, or <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this chapter from free book</a>.</li>\n<li><a href=\"https://github.com/pavankalyan1997/Machine-learning-without-any-libraries/blob/master/8.%20ANN/ANN.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Machine-learning-without-any-libraries</a> repo is designed to teach Machine Learning without any framework, no external dependencies (actually it requires: numpy, pandas, scikit-learn, matplotlib). In this example ANN has one hidden layer, tanh and sigmoid functions for activation, and backpropagation as learning algorithm. But it has wrong gradient for the output neuron. The way how functions pass values is far from readability, data loading can be orginized better, together with plotting functions it can eliminate all extra dependencies when only numpy will remain.</li>\n<li>Another <a href=\"https://github.com/ahmedfgad/NumPyANN/blob/master/TutorialProject/ann_numpy.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">NumPy only version</a>. My mistake, I started fiddling with ANN implementations by taking this code first. It doesn't have biases, no backpropagation, and no gradient descent in particular. Don't repeat my mistake.</li>\n<li>I don't know why all these examples are similar, like copied one from another without thinking, but <a href=\"https://towardsdatascience.com/how-to-build-a-deep-neural-network-without-a-framework-5d46067754d5\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this article</a> and <a href=\"https://github.com/marcopeix/Deep_Learning_AI/blob/master/1.Neural%20Networks%20and%20Deep%20Learning/4.Deep%20Neural%20Networks/dnn_utils_v2.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a> can be a good starting point. Someone tried to orginize code in small functions with some flexibility in mind. This and very space-y coding style make it hard to read. Take a look on the <code class=\"language-text\">cache</code> carried between forward and backward passes - it's very important concept for efficient implementation.</li>\n<li>In case you want to see how it can be orginized in classes, then here's <a href=\"https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">another post on Medium</a></li>\n<li>To do it clearly with no libraries, check out unbeatable and legendary Jason Brownlee <a href=\"https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">here</a>. He has <em>some</em> explanation about backpropaganation to the hidden layer (about multiplication on the weight matrix).</li>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/S0896627320307054\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">ANN Primer</a> for Neuroscientists. It covers the mathematical foundational aspects as well as the code for \"hands on\" experience. Around 30 examples are in <a href=\"https://github.com/gyyang/multitask\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">the repo</a> (written for Tensorflow 1.8.0, Python 2.7 / 3.6)</li>\n<li><a href=\"https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Why cross-entropy better than mean squared error</a> (better converges to 0 1 limits if we are using softmax on output)</li>\n<li>Play with another <a href=\"https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">cost functions</a></li>\n</ul>\n<h2 id=\"function-approximation\" style=\"position:relative;\">Function approximation<a href=\"#function-approximation\" aria-label=\"function approximation permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Then let's <a href=\"https://machinelearningmastery.com/neural-networks-are-function-approximators/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">approximate the sin function</a> and generalize our knowledge for any function. What requirements stand for functions in order to be approximable by neural networks?</p>\n<p>Later we will focus out research on aspects such as</p>\n<ul>\n<li>function sophistication (hypothesis: you get an excellent approximation but with apparently many more epochs needed)</li>\n<li>function properties: differentiable, continuous, smooth</li>\n<li>sinusoid and exponential (infinitely differentiable so no problems) -> approx with linear regression</li>\n<li>structural limitations of the network</li>\n<li>accuracy control</li>\n</ul>\n<p><a href=\"https://blog.cubieserver.de/2019/approximate-function-with-neural-network/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Here is a blog post</a> which tackles a few of the last bullet points.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; flex:1.2255639097744362;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dd432975049f1b3162e70e7af2644f8c/c391c/ann-function-approximation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.59509202453987%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACkElEQVR42m2T+08TQRDH+9/6ozHxN2M0/mCiMTGIIQaJggQDNpVCEChWK2JA0IDKo1yfu/fu3e3edbd7L9qrw7VWQL8/TGY3+5mZ3Z3J9FMlSQK22+2eper1euB2wjMRRLDf6yX9fwRIZrQIggBjjBAyDMOybdPQpQaS6g2XErPV8jzPdV3GGDjtdhscQDKDGJZlVSoVgMFxUqmaUa2jRqWqaDpxHNM0NU3T/wiOcc7PYahTURRICFEppUASSqUqxqeSLFUaSCXOJUEJhBAhxLDsOI47qWCLc+ayDlJNz7EtRVVVnXEOpY5gIMHC4SEchiFPBXQgBGGi3tRcWbFMG9cx91wIOmBG8LBsUBRFo8wg2SSyRVlL10ynjnSsO8K/Cvu+fxWGrTbjcg1pWovCC1lUdZgs6x3GSKoBbNs2PPhfGBaccT8ILFnFh8cE40bLs2hboVw+kUJDIXBt2x6FgJteujMUHfW66qnkNBstDUp2TSxjpCqVGsIa/PIIvpA5ScI4FlEEfeLWmj+PqrUaRrKhIFXVzLpJq9Wm9G0fesWhdNAIYEX62uet1+vA5yBRLlfezG8WtqQTqXxYxpubZmmjvPfjV1Xd+bh9UCgJl4oo9KOYCeGHQSYUHO1uZR8/yV+/tn7n5szY+Mr09MrM3Nri+v6Lyd1striwWFrIHU6OLU1MrMy+fjef/ZDLFXP57dX1zPvZ1y/vPpi9/+jV08m56fzOUuE09/brcnH7CB1I9b3949LeyZfPu5/Wivl84fnDZ+M3bi3fu70xNfV9tZjpNlFi2edTFUVhm4dRfOY4ARcR5z7rBG0WQIfYDrOIRzxGvFhREpv0CUkHIx3GpJ9cHLahTf4ziYMBhO9J+v3fXIBi7T9aVbQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"3d function approximation\"\n        title=\"\"\n        src=\"/static/dd432975049f1b3162e70e7af2644f8c/a6d36/ann-function-approximation.png\"\n        srcset=\"/static/dd432975049f1b3162e70e7af2644f8c/222b7/ann-function-approximation.png 163w,\n/static/dd432975049f1b3162e70e7af2644f8c/ff46a/ann-function-approximation.png 325w,\n/static/dd432975049f1b3162e70e7af2644f8c/a6d36/ann-function-approximation.png 650w,\n/static/dd432975049f1b3162e70e7af2644f8c/c391c/ann-function-approximation.png 673w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><a href=\"https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">This post</a> tries to explain <em>how</em> artificial neural networks work. So it is not about the algebraic part. Instead it tries to answer why size and structure matter, how the activation function makes a difference. And there I found the mesh neural networks interesting.</p>\n<h2 id=\"numerical-differentiation\" style=\"position:relative;\">Numerical differentiation<a href=\"#numerical-differentiation\" aria-label=\"numerical differentiation permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<h3 id=\"lotka-volterra-model\" style=\"position:relative;\">Lotka Volterra model<a href=\"#lotka-volterra-model\" aria-label=\"lotka volterra model permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p><a href=\"https://observablehq.com/@mbostock/predator-and-prey\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Theory and graphs</a></p>\n<p>So I'm trying to find Python library for numerical differentiation. numpy should do it, but I don't see an example for function of two variables f(x, y) that depend on time - x(t), y(t).</p>\n<p>For the multivariable thing you don't use numpy but <a href=\"https://docs.sympy.org/latest/tutorials/intro-tutorial/calculus.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">SymPy</a>.\n<a href=\"https://www.askpython.com/python/examples/derivatives-in-python-sympy\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">A more digestible tuto</a>.\nIt also explains how to use the chain rule</p>\n<p>For Lotka-Volterra ordinary differential equations</p>\n<ul>\n<li>sympy and scipi <a href=\"http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2015/tutorials/r6_sympy.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2015/tutorials/r6_sympy.html</a></li>\n<li>algebraic solution with sympy <a href=\"https://ipython-books.github.io/157-analyzing-a-nonlinear-differential-system-lotka-volterra-predator-prey-equations/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://ipython-books.github.io/157-analyzing-a-nonlinear-differential-system-lotka-volterra-predator-prey-equations/</a></li>\n<li>scipy with plot <a href=\"https://scientific-python.readthedocs.io/en/latest/notebooks_rst/3_Ordinary_Differential_Equations/02_Examples/Lotka_Volterra_model.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://scientific-python.readthedocs.io/en/latest/notebooks_rst/3_Ordinary_Differential_Equations/02_Examples/Lotka_Volterra_model.html</a></li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; flex:1.7717391304347827;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/275d9d0079bc00fcc121a60abe817f2a/29114/lotka-volterra-graph.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.44171779141104%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACeUlEQVR42h2RS08TURTHGxYsFKHQaUtnOu10ZjqPe+feeU8biIbEsEATNAREl4ofQeMXwI2BaNWmJWz4IArhkWh0zQJUHhEhpbalD9Lyar2Yc1Yn55fz/5+/D1m27qZN1xMVyEtAlKEMcJRlB6hAf4Dqp6iuri6KomiaDlJBv7/f7/cHgxQTjbJRxgeQqePh4fRt1xuSgclLWJDxi5evcvn820zmY/bD7OvZubk3uVxucXFhYSG3tLQ083xGVVVDxz4F6RhYEJoSaWCKqhEXwOraeqfTabcvT046V1edcrl9cdFuNDq16hmZZ7PZwUgEadAnAYQ0V4W2JGNZ0RXV4ES4vLJKlprN5vZWq9Vs7e5e1KvNwz/n+7s1Ms/n8zzPY6T5ZIAkxcLIs5CLNSup6AReWV2/PG01ivW9zerpUeXg52l1r3iwf17c3Cdw5l2GOCf89WUILd1MA+gQzwROiNraxsbleWfr63Flp/jjS6H0q7TzrVDYLu99/30Nv8/09PQkEtx/2djzUnck5EnQUYCVlPCnz8tX7fbfg+OzSrlWqtePDmuVRuu4UCldy56fn+/u7mbZKJGtITOtWWmIDEUzIDDIw++Ojo6N3x+7f+/hxMT4g/GJyclnM0+nnzyempqcnn7kuW44FKIZmkSlIyOtEd5wFGRryIHYE5JSgMTc1zfQ23vrxk2StwJFXmQ5nknwLBeLChzH8ZwPYNvwRqzUCDKHNN1VsKvitAwRHWODofBAgOrt89MMgzHSDR0hCIAS51gmykQig75UyrZMUpbhOq7jWI5r2zZCalwgKxF6MBQOUuFwSBT4RDwWZyJ8jJWFuCxwapL/B9Ik7dWhF+7SAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Lotka Volterra graph\"\n        title=\"\"\n        src=\"/static/275d9d0079bc00fcc121a60abe817f2a/a6d36/lotka-volterra-graph.png\"\n        srcset=\"/static/275d9d0079bc00fcc121a60abe817f2a/222b7/lotka-volterra-graph.png 163w,\n/static/275d9d0079bc00fcc121a60abe817f2a/ff46a/lotka-volterra-graph.png 325w,\n/static/275d9d0079bc00fcc121a60abe817f2a/a6d36/lotka-volterra-graph.png 650w,\n/static/275d9d0079bc00fcc121a60abe817f2a/e548f/lotka-volterra-graph.png 975w,\n/static/275d9d0079bc00fcc121a60abe817f2a/3c492/lotka-volterra-graph.png 1300w,\n/static/275d9d0079bc00fcc121a60abe817f2a/29114/lotka-volterra-graph.png 1920w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"logic-statements-in-ann\" style=\"position:relative;\">Logic statements in ANN<a href=\"#logic-statements-in-ann\" aria-label=\"logic statements in ann permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I've found two articles, more of a good read than a coding tutorial, it seems relevant:</p>\n<ol>\n<li><a href=\"https://towardsdatascience.com/emulating-logical-gates-with-a-neural-network-75c229ec4cc9\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Emulating logical gates with a neural network</a></li>\n<li><a href=\"https://medium.com/autonomous-agents/how-to-teach-logic-to-your-neuralnetworks-116215c71a49\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">How to teach logic to your neural networks</a>. And in this last link, you will find a link to <a href=\"https://playground.tensorflow.org/#activation=sigmoid&#x26;regularization=L2&#x26;batchSize=10&#x26;dataset=xor&#x26;regDataset=reg-plane&#x26;learningRate=0.03&#x26;regularizationRate=0&#x26;noise=0&#x26;networkShape=2,2,1&#x26;seed=0.00814&#x26;showTestData=false&#x26;discretize=false&#x26;percTrainData=90&#x26;x=true&#x26;y=true&#x26;xTimesY=false&#x26;xSquared=false&#x26;ySquared=false&#x26;cosX=false&#x26;sinX=false&#x26;cosY=false&#x26;sinY=false&#x26;collectStats=false&#x26;problem=classification&#x26;initZero=false&#x26;hideText=false\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">TensorFlow Playground</a>, that could help you visualize your network. And the associated <a href=\"https://github.com/tensorflow/playground\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">GitHub page</a></li>\n</ol>\n<h2 id=\"develop-new-actions\" style=\"position:relative;\">Develop new actions<a href=\"#develop-new-actions\" aria-label=\"develop new actions permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<blockquote>\n<p>If it uses fixed model of states and actions then it's not good. Is there a model that can develop new actions and adapt to the environment?</p>\n</blockquote>\n<p>I worked on <a href=\"https://www.allerin.com/blog/everything-you-need-to-know-about-adaptive-neural-networks\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">adaptive algos</a> for my med internship. it's the natural extension of ANNs apparently when coupled with the neural network approach</p>\n<p>A man-made adaptive neural network, also called an artificial neural network, is modeled after the naturally occurring neural networks in the brains of humans and animals.</p>\n<p>It's the same thing as an ANN pretty much</p>\n<p>Ok so the main use is for a single-layer algo. It's called <a href=\"https://www.allerin.com/blog/everything-you-need-to-know-about-adaptive-neural-networks\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Adaline</a> if you want</p>\n<p>Seems like a good start before having to deal with 5364 layers</p>\n<p>Activity Recognition with Adaptive Neural Networks - <a href=\"https://www.kaggle.com/code/malekzadeh/activity-recognition-with-adaptive-neural-networks\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Notebook</a> and <a href=\"https://arxiv.org/abs/2008.02397\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a></p>\n<p><a href=\"https://scikit-learn.org/stable/modules/neural_networks_supervised.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Multi Layer Perceptron</a> - I don't like this approach too much but just sending in case</p>\n<p>I think conventional \"training\" is wrong because it stops once errors on test set are minimized which obviously reveals the flaw - the network can only do what it trained to do), but I see that ANN can approximate functions, can follow logical statements, can store information as memory. It creates a base for my theory. Some insights can be borrowed from neuroscience to advance ANN quality, but there must be a way to transfer connections into symbolism - extract functions, logic and memories encoded in ANN. So what we just did, but in reverse.</p>\n<h2 id=\"bonus-reading\" style=\"position:relative;\">Bonus reading<a href=\"#bonus-reading\" aria-label=\"bonus reading permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p><a href=\"https://github.com/ArztSamuel/Applying_EANNs\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">This repo</a> simulates a car that mustn't touch walls. \"A 2D Unity simulation in which cars learn to navigate themselves through different courses. The cars are steered by a feedforward neural network. The weights of the network are trained using a modified genetic algorithm.\" Cars on C# is basically a \"game\" made on Unity game engine - hard to use as a standalone project. It was created 6 years ago - may be problems to run it on current Unity version. Also training by genetic algorithm is no go. The result is cool tho. (Unity, Genetic algorithm)</p>\n<p><a href=\"https://www.asimovinstitute.org/neural-network-zoo/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Neural Network Zoo</a> and <a href=\"https://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">a prequel</a> by Fjodor van Veen</p>\n<p>Just another Python library - PyGAD. It's focused on optimization algorithms and genetic algorithms. Specifically <a href=\"https://pygad.readthedocs.io/en/latest/nn.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">ANN module</a> can be interesting in regards the current topic.</p>\n<h2 id=\"best-notation\" style=\"position:relative;\">Best notation<a href=\"#best-notation\" aria-label=\"best notation permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/2107.09384.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Ostwald 2021</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Neilsen's book</a></li>\n<li><a href=\"https://arxiv.org/pdf/2004.04729.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Wiedemann 2020</a></li>\n<li><a href=\"https://arxiv.org/pdf/2202.06316.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Xiong 2022</a></li>\n<li><a href=\"https://arxiv.org/pdf/2110.06488.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Wang 2021</a></li>\n</ol>","excerpt":"In this post we collect known research about Artificial Neural Networks and their property to approximate non-linear functions and logical","tableOfContents":"<ul>\n<li>\n<p><a href=\"#backpropagation-formally-introduced\">Backpropagation formally introduced</a></p>\n</li>\n<li>\n<p><a href=\"#the-simpliest-ann\">The simpliest ANN</a></p>\n<ul>\n<li><a href=\"#pytorch\">Pytorch</a></li>\n<li><a href=\"#keras\">Keras</a></li>\n<li><a href=\"#tensorflow-no-keras\">Tensorflow (no Keras)</a></li>\n<li><a href=\"#no-frameworks\">No frameworks</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#function-approximation\">Function approximation</a></p>\n</li>\n<li>\n<p><a href=\"#numerical-differentiation\">Numerical differentiation</a></p>\n<ul>\n<li><a href=\"#lotka-volterra-model\">Lotka Volterra model</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#logic-statements-in-ann\">Logic statements in ANN</a></p>\n</li>\n<li>\n<p><a href=\"#develop-new-actions\">Develop new actions</a></p>\n</li>\n<li>\n<p><a href=\"#bonus-reading\">Bonus reading</a></p>\n</li>\n<li>\n<p><a href=\"#best-notation\">Best notation</a></p>\n</li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/6e5bdc7e-58d9-578a-8c49-5ee5cd4a543a.jpg"},"frontmatter":{"date":"October 25, 2022","published":"October 25, 2022","lastModified":"October 30, 2022","title":"Reinforcement Learning using ANN","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reinforcement-learning-using-artificial-neural-networks.md","url":"/ai/reinforcement-learning-using-artificial-neural-networks","next":{"excerpt":"Improvements First let's review improvements for Gradient Descent. They are still GD by its nature. Momentum gradient descent (MGD) Nesterov","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/alternatives-to-gradient-descent.md","frontmatter":{"title":"Alternatives to gradient descent","date":"2022-11-13T00:00:00.000Z","topic":null,"article":"main"},"id":"a39e9857-46a4-50e0-90aa-a814c61ddb0c"},"previous":{"excerpt":"First, a quick overview of current branches in machine learning (by Oleksii Trekhleb)  Biological neuron and mathematical models I started","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/computational-neuroscience.md","frontmatter":{"title":"Computational neuroscience","date":"2022-08-28T00:00:00.000Z","topic":null,"article":"main"},"id":"5a940991-7598-5841-8e1f-95491aa3e8fa"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}