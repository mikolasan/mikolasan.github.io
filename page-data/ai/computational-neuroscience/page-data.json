{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/computational-neuroscience","result":{"data":{"markdownRemark":{"html":"<p>First, a quick overview of current branches in machine learning (by <a href=\"https://github.com/trekhleb/homemade-machine-learning\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Oleksii Trekhleb</a>)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; flex:1.4298245614035088;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d9ba26bb7ee690d54b2a6e057ed87481/78415/machine-learning-map.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.93865030674846%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACZElEQVR42nVTa2/UMBC8//9j+I5AAiE+QAUtoD6AVtU1ba55Xez4HcfJsN7cUQrC0p2dtXe8OzPeLMuCZZ6R55QSzyEExBhhtIYxGjPtT9MEOQxoO4GqbqCU4jPOed7Pvzw2+WDTNLDWoSwfGezh/h4DJY/jhG7fQzOwQdtUCKqA7rfw3sFbhX37SGsLKSUXtCnLHZTsoNpL+KGgmxb0UlF1A8W+U2zLNzvnKKmH0yWCrbkTK7YQ1TeMXtLewF1slNJIU8BIh9I4IHkBWV4hmhbBUMz3DBjjxJXYjipsCgacJ4voGkjRoK5zlxYbHEc+QJPv77E9fYmoqIp14/DLI6G8eIfm+oQBPV1aF6dwqmFqMnVPgE+4v0nOSUZZKGEwp/X7SD5YTBLFaqZjOeQwYF5kMWYiNZMbxnEFJD6/frzAhzefiKdIsbTGc620DvIao7rhyrU2rPzmSHgvBPbFOe6+vEbQHSXQsTQR0YnUHlnBRBdMTmJ3/hZ6d8XAo6ng5Q0Cxb0PK2Ams207DG2BZnuOObqVuWXGGCycUbzOdCwkoKoIQO5ITE+iNfCqhFaCcxgwV5B9Fkn2lA1O7WQLLDRvz17h6v0LzIHcQGXnvZw40wWu/wknfiCOgVRu2egHUVYVUzTYP17CksJ55Hbj6Kkdi4lsk09NQcAQEJIhvtWhu5YLyqI9AxxEjfrhglrIr6Fgn+mhRk/eQ7Js6OR29FrusNA3U0B/3M2yYjyzjQ+kLsXNUKG8PaFXUUN1t9hXl2T6Paq7z2z2P4v4e/zjw3zbQK9HSM0tHX3HT82R2vy9/BfwF2RRO5wODwarAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ML roadmap\"\n        title=\"\"\n        src=\"/static/d9ba26bb7ee690d54b2a6e057ed87481/a6d36/machine-learning-map.png\"\n        srcset=\"/static/d9ba26bb7ee690d54b2a6e057ed87481/222b7/machine-learning-map.png 163w,\n/static/d9ba26bb7ee690d54b2a6e057ed87481/ff46a/machine-learning-map.png 325w,\n/static/d9ba26bb7ee690d54b2a6e057ed87481/a6d36/machine-learning-map.png 650w,\n/static/d9ba26bb7ee690d54b2a6e057ed87481/e548f/machine-learning-map.png 975w,\n/static/d9ba26bb7ee690d54b2a6e057ed87481/78415/machine-learning-map.png 1207w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"biological-neuron-and-mathematical-models\" style=\"position:relative;\">Biological neuron and mathematical models<a href=\"#biological-neuron-and-mathematical-models\" aria-label=\"biological neuron and mathematical models permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I started with an <a href=\"https://en.m.wikipedia.org/wiki/Neuron\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">article on Wikipedia about neuron</a>. I needed to clear all biological moments, understand new terminology, start with early research on the topic and catch up with the latest results.</p>\n<p>Phrase \"the most negative threshold potential\" led me to a specific article about <a href=\"https://en.m.wikipedia.org/wiki/Threshold_potential\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">threshold potential</a>. It reviews the neuron from electrical side.</p>\n<p>I should read more about <a href=\"/ai/neural-coding\"><strong>Neural coding</strong></a> in the future, but I couldn't resist to visit a citation that must explain how neuron can code digital and analog data: Thorpe SJ (1990). <a href=\"/ai/reviews/spike-arrival-times-a-highly-efficient-coding-scheme-for-neural-networks\">\"Spike arrival times: A highly efficient coding scheme for neural networks\"</a>. Funny, it's a small note that proposes use of time (relative time between spikes) for encoding. It looks as a simple change, but for ANN models it's something radically new. In respect of that I should read about <a href=\"/ai/spiking-neural-networks\">Spiking Neural Networks</a>. <strong>-></strong></p>\n<p>After pure biological overview it's time to glance at current models. <a href=\"https://en.wikipedia.org/wiki/Biological_neuron_model\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Biological neuron model</a></p>\n<p>The best model from 1952 is <a href=\"https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Hodgkin–Huxley model</a>. It received the Nobel prize in 1963. Based on squid neurons. Think about it ordering fried calamari next time.</p>\n<p>There are simplified models, but the main question is how to train them. For example <em>adaptive exponential integrate-and-fire model</em> (AdEx) and its implications described by Naud R, Marcille N, Clopath C, Gerstner W. (2008) <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798047/#!po=48.6842\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Firing patterns in the adaptive exponential integrate-and-fire model</a>. Despite it's a <a href=\"https://en.wikipedia.org/wiki/Exponential_integrate-and-fire\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">simple one-dimensional model</a>, it can simulate precisely many potential measurements graphs. And I find a <a href=\"https://neuronaldynamics-exercises.readthedocs.io/en/latest/exercises/adex-model.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Python library</a> for this model. I don't understand how they produce all different firing patterns. Is it just a play with model parameters or I missed a paragraph about training?</p>\n<p>I read <a href=\"https://link.springer.com/referenceworkentry/10.1007/978-3-540-92910-9_17\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Chapter 17</a> from <strong>Handbook of Natural Computing</strong> (2012), where I notice one big omission after reading Wikipedia. The synapse can be either electrical or chemical. This must be included into next improved roadmap version as the second step.</p>\n<p>What are the key examples of the use of computational methods in the study of biological neural networks? <a href=\"https://psychology.stackexchange.com/questions/1391/what-are-the-key-examples-of-the-use-of-computational-methods-in-the-study-of-bi?rq=1\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Q</a></p>\n<h2 id=\"network-organization\" style=\"position:relative;\">Network organization<a href=\"#network-organization\" aria-label=\"network organization permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Let's say we figured out neuron and synapse level, but how do we organize our network? Connections are not following the same pattern, exceptions are everywhere. (Read about <a href=\"https://www.science.org/stoken/author-tokens/ST-374/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><strong>Local connectivity and synaptic dynamics in mouse and human neocortex</strong></a>) Should we copy connections <a href=\"/ai/brain-map\">from a real brain</a>? Should we stick to one pattern? How many neurons to consider? I think here should be study about different neuron connections and their properties. Thus based on the qualities we need, we can construct a network satisfying our requirements. Also <a href=\"https://www.biorxiv.org/content/10.1101/2021.05.29.446289v4.full.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">A connectomic study of a petascale fragment of human cerebral cortex</a> tells that most likely an axon is connected to one synapse in another neuron. What's strange, volumetricly-speaking, axons take more space than dendrites on all layers, sometimes twice more.</p>\n<p>Authors of the book [Handbook of Natural Computing] see the answer to <em>how the network structure can influence the functionality of the system</em> in the work of Johnson S, Marro J, Torres JJ (2008) <a href=\"https://arxiv.org/abs/0805.1309\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Functional optimization in complex excitable networks</a>. Connections don't change, but neurons still can alter them by chemical control. This alteration happens constantly, but how do we preserve memories, our personality? Perhaps there are another levels that don't change. Is there a study that classifies NNs into such memory writers?</p>\n<p>To answer that there are metastable states in the work of Rabinovich MI, Huerta R, Varona P, Afraimovich VS (2008) <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000072\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Transient cognitive dynamics, metastability, and decision making</a>. Which I find rich for mathematical background and interesting for exercises, but not answering the question in terms of NNs, though interesting research to revisit later.</p>\n<p>Then I switch to a huge topic called <a href=\"https://www.nature.com/articles/1301559\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Synaptic plasticity</a>. Which actually brought me to a dead end. Not because the idea is dead, but because it introduces so many biological terms about chemical processes in neurons that I stop understanding this and other articles on the second sentence. I'll need to return to this topic when I will have interactive model ready. <strong>-></strong></p>\n<blockquote>\n<p>Neural plasticity, the alteration of function in response to experience, is a basic property of the nervous system that can be observed at the level of neural networks. Long-Term Potentiation (LTP) refers to a persistent enhancement of synaptic transmission following high frequency stimulation and is thought to reflect the cellular processes underlying learning and memory. The induction of LTP is dependent on activation of NMDA receptors, which are inhibited by organic solvents.</p>\n<p><em>Christoph van Thriel, William K. Boyes, in Advances in Neurotoxicology, 2022</em></p>\n</blockquote>\n<blockquote>\n<p>Plastic changes can also include the ability of the neurons to change their function and the amount and type of neurotransmitter they produce. [...] Although most of the 100 billion neurons are already formed at birth, neurons continue to make connections with other structures through dendritic branching and by remodeling other connections.</p>\n<p><em>Donna J. Cech DHS, PT, PCS, Suzanne “Tink” Martin MACT, PT, in Functional Movement Development Across the Life Span (Third Edition), 2012</em></p>\n</blockquote>\n<h2 id=\"feedback-loops\" style=\"position:relative;\">Feedback loops<a href=\"#feedback-loops\" aria-label=\"feedback loops permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Trying to digest information from the list above, I started thinking about possible neural connections between sensors and motors, I put one neuron in between and it already reminded me very simple perceptron. So instead of going the same route as Rosenblat and Minsky in 60s-80s, I raised my first question: How to connect the second neuron? Can it be connected in reverse that will make a loop? So I started looking for <em>neuron feedback loop</em>.</p>\n<p>Here is is my question <a href=\"https://psychology.stackexchange.com/questions/15932/are-there-neural-loops-within-a-column-or-an-area-of-the-cortex\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">asked by someon else</a> on StackExchange network. The short answer is yes, there are loops. And list of literature is provided.</p>\n<p><strong>The Mechanisms and Roles of Neural Feedback Loops for Visual Processing</strong> (2010) by Debajit Saha - a dissertation about <a href=\"https://openscholarship.wustl.edu/cgi/viewcontent.cgi?article=1306&#x26;context=etd\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">eyes of the turtle</a>, but mainly it leads me to a <a href=\"https://www.pnas.org/doi/10.1073/pnas.0712231105\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">scheme showing connections between neurons</a> and more details in the <a href=\"https://www.pnas.org/action/downloadSupplement?doi=10.1073%2Fpnas.0712231105&#x26;file=12231Appendix.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">appendix</a>.</p>\n<p>But again, I don't want to mimic connections from a real brain. Thus next I find a website that probably reviews everything that I found before so far and <a href=\"https://nba.uth.tmc.edu/neuroscience/m/s1/introduction.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">it gives classification to neuron connections</a></p>\n<p><strong>Evoked brain responses are generated by feedback loops</strong> - <a href=\"https://www.pnas.org/doi/10.1073/pnas.0706274105\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">feedback loop explained by bayesian model</a>. Not easy to understand, but it's a good reference to bayesian (probabilistic) models I think.</p>\n<p><a href=\"https://www.frontiersin.org/articles/10.3389/fncir.2012.00121/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">There are experiments</a> where to a culture of cortical neurons connect integrated circuit, read its signal and respond back. This way researchers can verify theories about plasticity, memory and training. This particular article doesn't get anything meaningful from neurons.</p>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2559979/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Training algorithm</a> to teach real neurons. Wild. Maybe I'll return back to this article, but as they write: \"Even though many details of cellular biology and electrode physics were absent from the model network...\".</p>\n<p><strong>DNN cyclic inter layer connections</strong>. <em>Deep neural networks using a single neuron: folded-in-time architecture using feedback-modulated delay loops</em> (<a href=\"https://www.nature.com/articles/s41467-021-25427-4\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.nature.com/articles/s41467-021-25427-4</a>)</p>\n<h2 id=\"piagets-theory-of-cognitive-development\" style=\"position:relative;\">Piaget's theory of cognitive development<a href=\"#piagets-theory-of-cognitive-development\" aria-label=\"piagets theory of cognitive development permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://en.m.wikipedia.org/wiki/Piaget%27s_theory_of_cognitive_development\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://en.m.wikipedia.org/wiki/Piaget%27s_theory_of_cognitive_development</a></li>\n<li>behavior. An artificial visual cortex drives behavioral evolution in co-evolved predator and prey robots <a href=\"https://dl.acm.org/doi/10.1145/2330784.2330838\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://dl.acm.org/doi/10.1145/2330784.2330838</a></li>\n</ul>\n<h2 id=\"multi-network-system\" style=\"position:relative;\">Multi network system<a href=\"#multi-network-system\" aria-label=\"multi network system permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Somewhere here I was interjected with a random recommendation. A speach from one conference in 2011 about <a href=\"http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/berlin_summit_2011.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">shifting the paradigm in robotics</a>. I found new interesting terms like <em>developmental psychology</em>, <em>morphological computation</em>, but they compare brain capabilities with current robot developments. I don't like such dull comparison when they highlight challenges in robotics and then say that the brain has more neurons but much slower and yet can solve all problems. Maybe because they formulate AI goal in a way that I think is absurd: here's input from camera - your eyes, here's other sensor metrics, now you need to be smart as human, so move to point B and avoid all obstacles. Why are you still standing?</p>\n<p>How to understand the goal? Even more, how to understand movements and think about solutions in unknown environment?</p>\n<p>Then I'm going back to two neurons that I drew in my notebook, I know that they can make a loop, and also I think that one neural network is not enough. I've read about ganglions - cluster of neurons that do preprocessing.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"flex:1.481818181818182;\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7776e5ce6fe3fe9316345e118d0f48ae/ae694/vestibular-system.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.48466257668711%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbklEQVR42m2SWW/TQBCA89v4C7zwC3jkAYF446FVpUpQcVZQHipacbWUJgg19FICKdArPRJCfKVxsm58u7Z3vY6vtVknRX1hNNqdPb6d2ZkpJElyYZqDo+1ebUUDZ1ku6WjMYhLg0CZpHCWJ6KKW6bpBSPdJml/QNK2QZFm3unQyfZ19fKMxPzFiL+E019yuQ23J65aCfsnqAuiOT3M4JJm59fR89YG1916vvEEO9AwjHfFD40I7O4Ni/0iXigSsk8G94saTbwcRoR4zlcIkzXTQNqqz2sqkXCvK+025dkASEkJbUHeabqWFKi32+7LYW3akqfXy7PEJJOE/mJBhFPff3QVzN+WvRe/OtDPxLBT6DmAafoklG/vp22O7/LAEZnZ6L5vVsgniOL6Ek5El80f9jRmwMHlx+771adNsNtTT01b9M8MsvlCvfRk+f3Soz9SFufYua6OrP9NsUytJM/DrY2f+Fr+/ZhkyGaWKPtu0dlfgVMX6sGpKiwZf60sOvizHFUwFY9w53uS2X/3eeq0pMl3msXmDKlqu47WOrVoRtnGqo/Q/cO6fJLrYMHp/koSmgozLJWGWdxsoyKNVdFPRzKuwfYR8x/Gwjz0vVJQoiIZh7I0F+4pGVH0Y+CHGPvI8x3FM04AQxlEEzs8Lriw7rVZPlocIwXJZ5XkRAEVRDUO3XW9hT/7ZtlVZGgxkACTbtuuNNsPxqqayHFfwaM9Zlu3mfQM9z0UojKJxk5AsEbMfQUz9uRBBiFCaEt3CEPm0Wl1RLPC8wDJsu82wLCdwvMB36EDtXDm+w0gc2xEEgR8p9UZnKt2ueHB4+BdqBrU2snwYnwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Primary and secondary afferents from the vestibular system to the cerebellum. \"\n        title=\"Sensing how to balance - [Scientific Figure on ResearchGate](https://www.researchgate.net/figure/Primary-and-secondary-afferents-from-the-vestibular-system-to-the-cerebellum-Neurons_fig1_332483580) [accessed 14 Sep, 2022]\"\n        src=\"/static/7776e5ce6fe3fe9316345e118d0f48ae/a6d36/vestibular-system.png\"\n        srcset=\"/static/7776e5ce6fe3fe9316345e118d0f48ae/222b7/vestibular-system.png 163w,\n/static/7776e5ce6fe3fe9316345e118d0f48ae/ff46a/vestibular-system.png 325w,\n/static/7776e5ce6fe3fe9316345e118d0f48ae/a6d36/vestibular-system.png 650w,\n/static/7776e5ce6fe3fe9316345e118d0f48ae/ae694/vestibular-system.png 850w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Sensing how to balance - <a href=\"https://www.researchgate.net/figure/Primary-and-secondary-afferents-from-the-vestibular-system-to-the-cerebellum-Neurons_fig1_332483580\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Scientific Figure on ResearchGate</a> [accessed 14 Sep, 2022]</p></figcaption>\n  </figure></p>\n<p><em>Multiple neural network systems</em>, but not when they are trained on different parameters and replace each other, but when they <a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.497.5717&#x26;rep=rep1&#x26;type=pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">must work in conjunction</a>. <strong>A theoretical framework for multiple neural network systems</strong> by Mike Shields and Matthew Casey. Graph theory and some algebra - very nerdy and heavy.</p>\n<p>For example Recurrent Neural Networks (RNN) have connection loops and specific memory nodes that together allows it to be Turing complete</p>\n<p>Also <a href=\"https://arxiv.org/pdf/1703.03400.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Model-Agnostic Meta-Learning</a> (MAML) look like a multi network system</p>\n<h2 id=\"temporal-encoding\" style=\"position:relative;\">Temporal encoding<a href=\"#temporal-encoding\" aria-label=\"temporal encoding permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ol start=\"22\">\n<li>Also temporal encoding of analog data is really intriguing topic that I want to figure out. Can be found in <a href=\"https://neuronaldynamics.epfl.ch/online/index.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this online book</a> that I immediately added to <a href=\"/science/reading-list\">my reading list</a>.</li>\n<li></li>\n<li>But then <a href=\"https://arxiv.org/pdf/1907.13223.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">research from Google team</a> very disappointed me. It must be a method of how to encode temporal data, that everyone citing to. And indeed, they encoded data, but just like programmers would do just to replace time with data that has weights that can be trained with very familiar practive of backpropagation.</li>\n</ol>\n<h2 id=\"sensorimotor\" style=\"position:relative;\">Sensorimotor<a href=\"#sensorimotor\" aria-label=\"sensorimotor permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p><a href=\"https://en.m.wikipedia.org/wiki/Sensory-motor_coupling\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Sensory-motor coupling</a></p>\n<ol start=\"23\">\n<li>I'll try to focus only on <a href=\"https://www.jneurosci.org/content/35/30/10888\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">motor information</a>. When I started reading this paper, I thought: Every robotics engineer must read this and think in such terms.</li>\n<li>There are already many attempts to create a neuron out of silicone (meaning by designing an integrated circuit) <a href=\"https://0795f079-a-62cb3a1a-s-sites.googlegroups.com/site/arindambasu/writings/2010_J2.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Nullcline-Based Design of a Silicon Neuron</a> that I found from in <a href=\"https://apps.dtic.mil/sti/pdfs/AD1101848.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">a big report</a> by Yang Yi: Enabling Brain-Inspired Processors Through Energy-Efficient Delayed Feedback Reservoir Computing Integrated Circuits.</li>\n</ol>\n<ul>\n<li>Coordination Dynamics in Cognitive Neuroscience - <a href=\"https://www.frontiersin.org/articles/10.3389/fnins.2016.00397/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.frontiersin.org/articles/10.3389/fnins.2016.00397/full</a></li>\n<li>Cortical coordination dynamics and cognition by Steven L. Bressler and J.A. Scott Kelso <a href=\"http://www.ccs.fau.edu/~bressler/pdf/TICS01.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">http://www.ccs.fau.edu/~bressler/pdf/TICS01.pdf</a> (found at <a href=\"https://www.semanticscholar.org/paper/Cortical-coordination-dynamics-and-cognition-Bressler-Kelso/1a10508b6fdc44a2f1cc0a62659b0c87e46f8642\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.semanticscholar.org/paper/Cortical-coordination-dynamics-and-cognition-Bressler-Kelso/1a10508b6fdc44a2f1cc0a62659b0c87e46f8642</a>)</li>\n<li>Toward a self-organizing pre-symbolic neural model representing sensorimotor primitives <a href=\"https://www.frontiersin.org/articles/10.3389/fnbeh.2014.00022/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.frontiersin.org/articles/10.3389/fnbeh.2014.00022/full</a></li>\n</ul>\n<p><a href=\"/ai/reservoir-computing\">Reservoir Computing</a> sounds very interesting. I've heard about it and abiout <em>Liquid State Machines</em> when I was doing a search about Finite State Machines, and someone probably by mistake attached it into related articles.</p>\n<p>The paper says that reservoir computing is good, essentially it just converts the problem to a higher dimension (why? when everyone tries to simplify the problem instead), implementation in software is slow, digital circuits are slow too, so let's do analog systems. And they provide circuits and explanation. Good, but no extensive comparison and explanation of reservoir benefits in the beginning. Only some simple tests in the end.\n24. <a href=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Neural networks and manifolds</a>, <a href=\"https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">visualization of the learning process</a>\n25. 2001 S. Bressler, J. Kelso - Cortical coordination dynamics and cognition <a href=\"http://www.ccs.fau.edu/~bressler/pdf/TICS01.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">PDF</a></p>\n<ul>\n<li><a href=\"https://quantdare.com/understanding-neural-networks-with-graphs/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">ANN 101</a></li>\n<li><a href=\"https://web.archive.org/web/20160221220505/http://tocs.ulb.tu-darmstadt.de/110876539.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Amazing NN catalogue</a> but in German only</li>\n<li><a href=\"https://www.researchgate.net/publication/352493209_User_Driven_FPGA-Based_Design_Automated_Framework_of_Deep_Neural_Networks_for_Low-Power_Low-Cost_Edge_Computing\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">DNN with cyclic connections</a></li>\n<li><a href=\"https://openreview.net/pdf?id=S1xSzyrYDB\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2020 Cyclic graph multilayer perceptron</a> - bleeding edge</li>\n<li><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Graph Neural Networks</a></li>\n<li>2021 <a href=\"https://jiechenjiechen.github.io/pub/dagnn.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">DIRECTED ACYCLIC GRAPH NEURAL NETWORKS</a></li>\n<li>I believe that there is a homomorphism between ANN and SNN. Wait, but can xNN be defined as an algebraic structure? Of course: <a href=\"https://www.semanticscholar.org/paper/Modelling-a-neural-network-using-an-algebraic-Sugunnasil-Somhom/3de6d2dd204958731f0d3eeccdc317547b5a9413\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Modelling a neural network using an algebraic method</a></li>\n<li><a href=\"https://arxiv.org/pdf/2009.01433.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Algebraic Neural Networks</a></li>\n<li>Very optimistic article about <a href=\"https://www.semanticscholar.org/paper/Reinforcement-learning-in-artificial-and-biological-Neftci-Averbeck/951af7222535d934ca2b401ca0cd2181b28284f9\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Reinforcment learning</a>. At the end it says that SNN will change the world and overthrow Moore's law. But it answers many questions about neverending learning and about using the same neurons on different timescales - by different systems (learning vs predicting)</li>\n</ul>\n<h2 id=\"embodied-cognition\" style=\"position:relative;\">Embodied Cognition<a href=\"#embodied-cognition\" aria-label=\"embodied cognition permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://plato.stanford.edu/entries/embodied-cognition/#FoilInspForEmboCogn\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://plato.stanford.edu/entries/embodied-cognition/#FoilInspForEmboCogn</a></li>\n</ul>\n<h2 id=\"lstm\" style=\"position:relative;\">LSTM<a href=\"#lstm\" aria-label=\"lstm permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/</a></li>\n<li><a href=\"https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/</a></li>\n<li>keras implementation <a href=\"https://keras.io/guides/working_with_rnns/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://keras.io/guides/working_with_rnns/</a></li>\n<li>original paper (1997 Hochreiter Schmidhuber)  <a href=\"https://www.bioinf.jku.at/publications/older/2604.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.bioinf.jku.at/publications/older/2604.pdf</a></li>\n<li>panda timeseries <a href=\"https://pandas.pydata.org/docs/dev/getting_started/intro_tutorials/09_timeseries.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://pandas.pydata.org/docs/dev/getting_started/intro_tutorials/09_timeseries.html</a></li>\n</ul>\n<h2 id=\"terminology\" style=\"position:relative;\">Terminology<a href=\"#terminology\" aria-label=\"terminology permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>metastable cognitive states/modes <a href=\"https://www.fil.ion.ucl.ac.uk/~karl/Transients,%20Metastability,%20and%20Neuronal%20Dynamics.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">article</a></li>\n<li>transient cognitive (brain) dynamics</li>\n<li>read about layers in <a href=\"https://en.wikipedia.org/wiki/Cerebral_cortex\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Cerebral cortex</a> article</li>\n<li>in case you are bored looking at neurons, then extend your view by reviewing <em>extended mind thesis</em></li>\n<li>temporal neural encoder</li>\n<li>neuromorphic computing systems</li>\n<li><a href=\"https://www.frontiersin.org/articles/10.3389/fncom.2019.00082/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">spike coded trains</a> - the sequence of neuronal firing timings</li>\n<li><a href=\"https://www.jeremyjordan.me/autoencoders/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">autoencoders</a> - hidden layers in ANN, output is equal to input and learning formula L is special. Boring.</li>\n<li>Developmental psychology</li>\n<li>developmental learning</li>\n<li>Morphological computation</li>\n<li>Moravec's paradox</li>\n<li>symbolic AI</li>\n<li>Comparative connectomics</li>\n<li>Stage of cognitive development</li>\n</ul>\n<h2 id=\"notes\" style=\"position:relative;\">Notes<a href=\"#notes\" aria-label=\"notes permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Use numerical differentiation for proving that over time the networks does not exponentially explode with cycles and other unpredictable activities. Most likely it’s not helpful during training, but can be used for upper bound evaluation.</p>\n<h2 id=\"quotes\" style=\"position:relative;\">Quotes<a href=\"#quotes\" aria-label=\"quotes permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<blockquote>\n<p>Symmetric reciprocal inhibition leads to multistability and this is not an appropriate dynamical regime for the description of reproducible transients. As we have shown [...], <strong>nonsymmetric inhibition is an origin of reproducible transients</strong> in neural networks.</p>\n<p>Rabinovich et al, 2008</p>\n</blockquote>\n<blockquote>\n<p>As we’ve mentioned in sections above, we ultimately want to get away from the dataset-based approach...</p>\n<ul>\n<li>How can one reliably and naturally communicate a task to the system?</li>\n<li>Could there be some hybrid approaches with algorithmic logic on top of perceptual primitives (similar to Yang et al. (2015))?</li>\n</ul>\n<p>How can a primarily unsupervised machine interact well with humans?\nMany of these robotics questions have parallels within the context of neuroscience/cognitive science. As much as neuroscience may not be the easiest way to get these answers (due to the inherent complexity of the brain, and the difficulty in isolating the relevant effects), some convergence of the fields might yield fruitful insights.</p>\n<p>Filip Piekniewski et al, 2016</p>\n</blockquote>\n<p><a href=\"https://twitter.com/filippie509/status/1575512408566603776\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://twitter.com/filippie509/status/1575512408566603776</a></p>\n<blockquote>\n<p>I think what is missing here is the “survival” module.  The only sentient systems we know are in perpetual survival mode. All their qualities, including sentience and reproduction are there to assure survival.</p>\n<p>Anonymous <a href=\"https://twitter.com/locoqf2/status/1581326320738238464\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://twitter.com/locoqf2/status/1581326320738238464</a></p>\n</blockquote>\n<h2 id=\"literature\" style=\"position:relative;\">Literature<a href=\"#literature\" aria-label=\"literature permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://web.archive.org/web/20120215151304/http://pop.cerco.ups-tlse.fr/fr_vers/documents/thorpe_sj_90_91.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">1990 Thorpe</a> Spike arrival times: A highly efficient coding scheme for neural networks</li>\n<li>Naud R, Marcille N, Clopath C, Gerstner W. (2008) <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798047/#!po=48.6842\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Firing patterns in the adaptive exponential integrate-and-fire model</a></li>\n<li>Handbook of Natural Computing. Chapter 17. Modeling Biological Neural Networks</li>\n<li>Samuel Johnson, J. Marro, and Joaqu ́ın J. Torres <strong>Functional Optimization in Complex Excitable Networks</strong> <a href=\"https://arxiv.org/abs/0805.1309\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">arxiv</a></li>\n<li>Rabinovich MI, Huerta R, Varona P, Afraimovich VS (2008) <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000072\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Transient cognitive dynamics, metastability, and decision making</a></li>\n<li><a href=\"https://apps.dtic.mil/sti/pdfs/AD1101848.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2020 Comsa</a> - Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function Learning with Backpropagation</li>\n<li><a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.497.5717&#x26;rep=rep1&#x26;type=pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2008 Shields Casey</a> - A theoretical framework for multiple neural network systems</li>\n<li><a href=\"https://www.jneurosci.org/content/jneuro/35/30/10888.full.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2015 Vargas-Irwin, Franquemont, Black, Donoghue</a> - Linking Objects to Actions Encoding of Target Object and Grasping Strategy in Primate Ventral Premotor Cortex</li>\n<li><a href=\"https://sites.google.com/site/arindambasu/writings/2010_J2.pdf?attredirects=1\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2010 Basu Hasler</a> - Nullcline-Based Design of a Silicon Neuron</li>\n<li><a href=\"https://arxiv.org/abs/1607.06854\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2016 Filip Piekniewski et al</a> Unsupervised Learning from Continuous Video in a Scalable Predictive Recurrent Network</li>\n<li>2001 Bressler Kelso - Cortical coordination dynamics and cognition</li>\n<li><a href=\"https://arxiv.org/pdf/1404.7828.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2016</a> Deep Learning in Neural Networks: An Overview</li>\n</ul>\n<h2 id=\"links\" style=\"position:relative;\">Links<a href=\"#links\" aria-label=\"links permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://web.archive.org/web/20121204133835/http://ctnsrv.uwaterloo.ca/cnrglab/node/215\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Neural Engineering Framework</a></li>\n<li><a href=\"https://robomimic.github.io/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">robomimic</a> is a framework for robot learning from demonstration</li>\n<li><a href=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">nice blog</a></li>\n<li><a href=\"https://maziarraissi.github.io/research/7_multistep_neural_networks/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">nice post</a></li>\n<li>Logical approach <a href=\"https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">McCulloch &#x26; Pitt's article: \"A logical calculus of the ideas immanent in neural activity\"</a></li>\n</ul>","excerpt":"First, a quick overview of current branches in machine learning (by Oleksii Trekhleb)  Biological neuron and mathematical models I started…","tableOfContents":"<ul>\n<li><a href=\"#biological-neuron-and-mathematical-models\">Biological neuron and mathematical models</a></li>\n<li><a href=\"#network-organization\">Network organization</a></li>\n<li><a href=\"#feedback-loops\">Feedback loops</a></li>\n<li><a href=\"#piagets-theory-of-cognitive-development\">Piaget's theory of cognitive development</a></li>\n<li><a href=\"#multi-network-system\">Multi network system</a></li>\n<li><a href=\"#temporal-encoding\">Temporal encoding</a></li>\n<li><a href=\"#sensorimotor\">Sensorimotor</a></li>\n<li><a href=\"#embodied-cognition\">Embodied Cognition</a></li>\n<li><a href=\"#lstm\">LSTM</a></li>\n<li><a href=\"#terminology\">Terminology</a></li>\n<li><a href=\"#notes\">Notes</a></li>\n<li><a href=\"#quotes\">Quotes</a></li>\n<li><a href=\"#literature\">Literature</a></li>\n<li><a href=\"#links\">Links</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/5a940991-7598-5841-8e1f-95491aa3e8fa.jpg"},"frontmatter":{"date":"August 28, 2022","published":"October 03, 2022","lastModified":"December 19, 2022","title":"Computational neuroscience","subtitle":"The roadmap","section":"brain","draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/computational-neuroscience.md","url":"/ai/computational-neuroscience","next":{"excerpt":"In this post we collect known research about Artificial Neural Networks and their property to approximate non-linear functions and logical…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reinforcement-learning-using-artificial-neural-networks.md","frontmatter":{"title":"Reinforcement Learning using ANN","date":"2022-10-25T00:00:00.000Z","topic":null,"article":"main"},"id":"6e5bdc7e-58d9-578a-8c49-5ee5cd4a543a"},"previous":{"excerpt":"It probably started in 1982. Full crash course about Hopfiled networks. Train Predict Full notebook on AI sandbox More resources An example…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/hopfield-networks.md","frontmatter":{"title":"Hopfield Networks","date":"2022-08-17T00:00:00.000Z","topic":true,"article":null},"id":"3b572954-08ca-5093-80f2-7893c124ff94"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}