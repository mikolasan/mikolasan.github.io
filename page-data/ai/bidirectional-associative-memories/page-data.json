{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/bidirectional-associative-memories","result":{"data":{"markdownRemark":{"html":"<p>BAM can link together data of different types. Associations...</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; flex:1.4954128440366972;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fa96bc62d778051ca054ad89c8992c6c/cd78c/bam-picture-text-pairs.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.87116564417178%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACVUlEQVR42k1SS28SYRTlh7nr3o2bLtiqi8YaIfG5MJrWmNQYq2nT4qImrYkimoIBeZSHUF6FDo8O5VkGkBQBh2EGGOw8GM/MYPVkcnO/ud/57rkPgyzLvzVMp1Oe5+GwLMtxHI7wLzToIdjJZIIoLEIgGmay3Ol0yGLxR7uNG4qi8BMViI3HY3Y4zJNkvdEARxQFURBG3EgUJf1RA27nSNJ484bbf3B+3okmEtX6WalWLZSKQ5blJ/y6Zcvq2IeWfIEsVso1ijqtlH92u7PZTCWXa7Wri4tfXa5Gs/nhs83hcrn9frfP1+v3hQvhxebG291dkD0Bv8P9zenz2p3OcrUKokrO5vNLJhM4kIr3FHwKzAxV0DT9xmKx2vchW5Ik/T8sNAuCoJJR1a17d11eDz0Y+ELBcDz2PRqNHiUHzACNevj0yevtLZDdB75Uhkgep6PJZKvdnssunBavGY1fHA6GYQKRcDASicTjsL9oGs8vmU2PV1ch2x8KhWOxRDqF0BlFzWVnsrk7D+6jSF22rhxTQM/BWTabn62toQRRFC9l61PUZBcK128vH4RCdYp6/8nqD4fRG28wgMy48Xz91fa7HRS/s7fnCQSCh4c2u71UqcwzHxPElYWFjzYbZKcJIntygjkRueyAYZDh0crKy82NEcfFEvF0NkPkcvHUUbPVmtcMTiqV7vZ68GVJVjQgJmpAEqrRQEXYDeUv0Hl1w/4/sxpQKlZR3UZtRWVJQtvgDIcI8f/6osvWD9juXq/PsRzDYLU4bcEhVgVGyI1U4B/Il/n+AGuGpxpYPaOYAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Patterns pairs\"\n        title=\"\"\n        src=\"/static/fa96bc62d778051ca054ad89c8992c6c/a6d36/bam-picture-text-pairs.png\"\n        srcset=\"/static/fa96bc62d778051ca054ad89c8992c6c/222b7/bam-picture-text-pairs.png 163w,\n/static/fa96bc62d778051ca054ad89c8992c6c/ff46a/bam-picture-text-pairs.png 325w,\n/static/fa96bc62d778051ca054ad89c8992c6c/a6d36/bam-picture-text-pairs.png 650w,\n/static/fa96bc62d778051ca054ad89c8992c6c/e548f/bam-picture-text-pairs.png 975w,\n/static/fa96bc62d778051ca054ad89c8992c6c/cd78c/bam-picture-text-pairs.png 1236w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>From one side the model requires to use bipolar patterns - arrays of -1 and 1. But I need to store words and sentences. How do I decode them into that format? I looked into byte pair encoding (BPE) used in GPT-2. It basically tokenized by subwords.</p>\n<p>How letters and words stored in neurons? Should I look into psychological research or just made up some decoder?</p>\n<ul>\n<li>Cannot achieve clusters and hierarchy with just one layer in BAM.</li>\n<li>Sadly words are triggered together with other neuron activity (even with electrodes placed in the brain, 'electrocorticography' - official name of the technique, we can't get much understanding)</li>\n<li>Every letter and every word stored in separate neurons.</li>\n</ul>\n<blockquote>\n<p>GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.</p>\n<p>GPT-2 uses bytes as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that every base character is included in the vocabulary. GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.</p>\n<p>From <a href=\"https://huggingface.co/docs/transformers/tokenizer_summary\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://huggingface.co/docs/transformers/tokenizer_summary</a></p>\n</blockquote>\n<p>Here is how BPE encoding looks like. You can see it as a dictionary used by GPT. It has subwords and single letters. But the amount of subwords is limited by researchers.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; flex:1.304;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f2d17d8148b58c16304ef78fd554cfc9/30c92/bpe-encoding-gpt-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 76.68711656441718%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABw0lEQVR42j2SC3KkMAxEuc3uJsPPX7ANA0wm9z9SniSyVSpjC7XUaqnr59Xlp8ubz8fo22MuvavY4OpjWuR5WzV/7wpOs+7Pp5tCm8M2+jLFNvo6xw3PGBrPKcgTJ3/HUH1+ck5hc2kf3Nr96yMun/dJ/1m0X7g0GAkyVEJ5YqmcZMEf1xNqCl4O6pA+LM/RyT+iCQrir9THP+BPu7IoXpK2wbfuc0i5vsJ68GNpX1CgbKqvOW2c4MFwAcBd/JH6V1yPKe7dxw0+CcrlxckdevMvBiPaK9WoYbGcgeL5ABxvhXwxtqKZdkHx+6maqX4NnXhiNCKV4QxVS+mgrQypzGl+4ZV3vcC2eRMlNAEbK2Qz2Lp9EyqaZ8WrkyZd2qJ0dMGILOja/X0E4zDr6AZfYrmGm23r3WpmM2dDfskXoW1zdlLkJJrLqKOmmkw+7boCzfaHyqgLjHUE1X2Mic+yvYmGTG5floIudDaXE7xILX7R5cowR+O4CW2Q/9XGq5pXq0Nj+GnSNkzDmtPpDL7KqEhmP2QNY1OpnpZIF1YIa4CcOr/qZG3ZsDEt7S2hSsmJkqculrXdYnkxMGGUJbWtnXRUrh9K4sAfkkBjuwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"BPE encoding\"\n        title=\"\"\n        src=\"/static/f2d17d8148b58c16304ef78fd554cfc9/a6d36/bpe-encoding-gpt-2.png\"\n        srcset=\"/static/f2d17d8148b58c16304ef78fd554cfc9/222b7/bpe-encoding-gpt-2.png 163w,\n/static/f2d17d8148b58c16304ef78fd554cfc9/ff46a/bpe-encoding-gpt-2.png 325w,\n/static/f2d17d8148b58c16304ef78fd554cfc9/a6d36/bpe-encoding-gpt-2.png 650w,\n/static/f2d17d8148b58c16304ef78fd554cfc9/30c92/bpe-encoding-gpt-2.png 874w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>As a result they have flat vocabulary. By \"flat\" I mean all symbols are independent. Strange prefix \\u0120 (sometimes printed as Ġ, [<a href=\"https://github.com/huggingface/transformers/issues/3867\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">1</a>], [<a href=\"https://github.com/pytorch/fairseq/issues/1716\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2</a>], [<a href=\"https://github.com/openai/gpt-2/issues/80\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">3</a>]) denotes a space, which means that nothing can be prepended to symbols containing it.</p>\n<p>But instead of being flat we need a hierarchical network that starts with single letters and combines them into subwords and words.</p>\n<p>Next, firing of inputs is not \"flat\" as well. We will not present a word \"hello\" as a simultaneous input to first layer neurons</p>\n<h2 id=\"papers\" style=\"position:relative;\">Papers<a href=\"#papers\" aria-label=\"papers permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>Original - <a href=\"https://sipi.usc.edu/~kosko/BAM.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Kosko (1988)</a> and a <a href=\"https://sipi.usc.edu/~kosko/ABAM.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">follow up with more examples</a></li>\n<li><a href=\"http://jpbachy.free.fr/chartier-TNNc.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Chartier</a> uses two matrices instead of transposition of one matrix (<a href=\"https://www.sciencedirect.com/science/article/pii/S1877050918323901?ref=pdf_download&#x26;fr=RR-2&#x26;rr=7a894a268de97c7d\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">another short paper</a>)</li>\n<li>some explanation with code and pictures (but wrong tests, meaningless random data) <a href=\"https://towardsdatascience.com/a-succinct-guide-to-bidirectional-associative-memory-bam-d2f1ac9b868\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">post</a></li>\n<li>Tae-Dok Eom, Changkyu Choi, Ju-Jang Lee <a href=\"https://citeseerx.ist.psu.edu/document?repid=rep1&#x26;type=pdf&#x26;doi=7eb4f5ed9fb4cf20d63dff3e6c24f8f2aac35670\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Generalized asymmetrical bidirectional associative memory for multiple association</a></li>\n<li><a href=\"http://www.cyber-s.ne.jp/Top/Volume/1-1/0001tf.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">short version</a> (still don’t understand what their notation means)</li>\n<li><a href=\"https://www.hindawi.com/journals/jam/2011/301204/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">more examples and explanation</a> of chartier approach, good examples with text and picture associations. and more: temporal pattern sequences with the help of autoassociative layer (<a href=\"https://downloads.hindawi.com/journals/jam/2011/301204.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">pdf</a>)</li>\n<li><a href=\"https://www.mdpi.com/2073-8994/14/2/216\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">very formal and formula rich</a>, mostly about stability</li>\n<li><a href=\"https://arxiv.org/abs/2211.09694\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">another paper</a> with many formulas, this time focused on memory capacity</li>\n</ul>","excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","tableOfContents":"<ul>\n<li><a href=\"#papers\">Papers</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/3ae9ac87-4c8b-5f18-8f19-ee1091111f95.jpg"},"frontmatter":{"date":null,"published":"March 15, 2023","lastModified":"March 19, 2023","title":"Bidirectional Associative Memories","subtitle":null,"section":"brain","draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","url":"/ai/bidirectional-associative-memories","next":null,"previous":{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}