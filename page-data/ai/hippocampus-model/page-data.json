{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/hippocampus-model","result":{"data":{"markdownRemark":{"html":"<p>Ref: <a href=\"https://www.youtube.com/watch?v=EC6e8y4_nBs\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Modeling of Hippocampus Together with Neocortex for Few-Shot Learning</a> by Gideon Kowadlo from Cerenaut.ai</p>\n<h2 id=\"dg\" style=\"position:relative;\">DG<a href=\"#dg\" aria-label=\"dg permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I have a layer that transforms an input into a sparse tensor.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DGModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"\n    Goal: separation\n    Methods: sparsity\n    \"\"\"</span>\n    \n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DGModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n        nn<span class=\"token punctuation\">.</span>init<span class=\"token punctuation\">.</span>xavier_uniform_<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc1<span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">)</span>\n        \n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n            x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tanh<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x</code></pre></div>\n<p>But the result of this network is very noisy tensor with many cells active at the same time. How to keep the sparsity effect which means that even a small difference in input creates a different output but also it has very little of active cells. Maybe the same amount of active cells as in the input.</p>\n<h2 id=\"memory\" style=\"position:relative;\">Memory<a href=\"#memory\" aria-label=\"memory permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Memory is done with a <a href=\"/ai/hopfield-networks\">Hopfield network</a></p>\n<h2 id=\"notebooks\" style=\"position:relative;\">Notebooks<a href=\"#notebooks\" aria-label=\"notebooks permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://codeberg.org/mikolasan/ai-sandbox/src/branch/master/arc/kohonen.ipynb\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">SOM</a></li>\n<li><a href=\"https://codeberg.org/mikolasan/ai-sandbox/src/branch/master/arc/Hippocampus.ipynb\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Hippocampus</a></li>\n</ul>\n<h2 id=\"training\" style=\"position:relative;\">Training<a href=\"#training\" aria-label=\"training permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I don’t like how training works right now. It applies the whole matrix, and it does that constantly. And initial random weights can be negative and positive.</p>\n<p>I think they all should be positive only and have little values in the beginning.</p>\n<p>And every neuron will have a big threshold. During training this threshold can be lowered and values increased.</p>\n<p>For example, you’re listening speech in another language and maybe you’ll learn some words and you can recognize them, but other words you cannot recognize, and they will go to some empty new category. For every new word. And if you were listening for a speech (or reading long text) with many words, then possibly big amount of new words would create many new categories. So many that, of course, it will explode.</p>\n<p>But this would happen in the hippocampus, which means in a short term memory these categories can be overwritten. So some words will be erased, but if you repeat the words again then you will build categories that will go to a long-term memory. There they will be stored properly. What about <a href=\"/blog/should-ai-sleep\">sleep functions</a> in remembering?</p>","excerpt":"Ref: Modeling of Hippocampus Together with Neocortex for Few-Shot Learning by Gideon Kowadlo from Cerenaut.ai DG I have a layer that…","tableOfContents":"<ul>\n<li><a href=\"#dg\">DG</a></li>\n<li><a href=\"#memory\">Memory</a></li>\n<li><a href=\"#notebooks\">Notebooks</a></li>\n<li><a href=\"#training\">Training</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/5852ce15-bb47-5444-9198-3420aca715e0.jpg"},"frontmatter":{"date":"October 01, 2024","published":"October 14, 2024","lastModified":"October 14, 2024","title":"Modelling of Hippocampus","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/hippocampus-model.md","url":"/ai/hippocampus-model","next":{"excerpt":"Negative lessons are very important. And they supposed to be stored very specifically in its own categories. These categories will provide…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/negative-training.md","frontmatter":{"title":"Negative training","date":"2024-12-07T00:00:00.000Z","topic":true,"article":null},"id":"8bdcd2d5-dd95-5f5b-b1d6-49a931e2c290"},"previous":{"excerpt":"What are invariants in ANN? If some features extracted from the input data stay unchanged under some types of transformation like rotation…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/understanding-invariants-in-ann.md","frontmatter":{"title":"Understanding invariants in ANN","date":"2024-09-23T00:00:00.000Z","topic":null,"article":"quest"},"id":"ee719911-0ae9-5071-8cba-5279bfb73a3f"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}