{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/reviews/workink-memory-encoding-mechanism-explained","result":{"data":{"markdownRemark":{"html":"<p>LINK: <a href=\"https://www.nature.com/articles/s42003-024-07282-3\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">nature.com</a></p>\n<h2 id=\"how-i-found-it\" style=\"position:relative;\">How I found it<a href=\"#how-i-found-it\" aria-label=\"how i found it permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I was looking for some insights about encoding methods and known tricks like spatial distribution in sparse encoding while I was playing with <a href=\"/ai/hippocampus-model\">hippocampus model</a>.</p>\n<h2 id=\"what-i-expect-after-reading-the-abstract\" style=\"position:relative;\">What I expect after reading the abstract<a href=\"#what-i-expect-after-reading-the-abstract\" aria-label=\"what i expect after reading the abstract permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>RNN example with sparse encoding.</p>\n<p>How information stored in neurons? Every neuron has a value? Every dendrite has a value? Or memory is not stored in one neuron but rather in a group of neurons when the path of activations encodes a specific pattern (this is a term from signal theory, but how would we call a single entity stored in memory?) This paper says that it’s both. So can we extract one entity? Can we find and erase specific memories?</p>\n<p>And what is stored in memory? Short little facts or chains of events?</p>\n<p>The main question is what data we need to train such memory and how this stored information is used?</p>\n<h2 id=\"abstract\" style=\"position:relative;\">Abstract<a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<blockquote>\n<p>Whether working memory (WM) is encoded by persistent activity using attractors or by dynamic activity using transient trajectories has been debated for decades in both experimental and modeling studies, and a consensus has not been reached. Even though many recurrent neural networks (RNNs) have been proposed to simulate WM, most networks are designed to match respective experimental observations and show either transient or persistent activities. Those few which consider networks with both activity patterns have not attempted to directly compare their memory capabilities. In this study, we build transient-trajectory-based RNNs (TRNNs) and compare them to vanilla RNNs with more persistent activities. The TRNN incorporates biologically plausible modifications, including selfinhibition, sparse connection and hierarchical topology. Besides activity patterns resembling animal recordings and retained versatility to variable encoding time, TRNNs show better performance in delayed choice and spatial memory reinforcement learning tasks. Therefore, this study provides evidence supporting the transient activity theory to explain the WM mechanism from the model designing point of view.</p>\n</blockquote>","excerpt":"LINK: nature.com How I found it I was looking for some insights about encoding methods and known tricks like spatial distribution in sparse…","tableOfContents":"<ul>\n<li><a href=\"#how-i-found-it\">How I found it</a></li>\n<li><a href=\"#what-i-expect-after-reading-the-abstract\">What I expect after reading the abstract</a></li>\n<li><a href=\"#abstract\">Abstract</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/6f25ccb6-7d1e-5fd3-8fd2-368a5adc24bf.jpg"},"frontmatter":{"date":null,"published":"January 02, 2025","lastModified":"June 23, 2025","title":"Recurrent neural networks with transient trajectory","subtitle":"explain working memory encoding mechanisms","section":"brain","draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/workink-memory-encoding-mechanism-explained.md","url":"/ai/reviews/workink-memory-encoding-mechanism-explained","next":{"excerpt":"PDF: https://arxiv.org/pdf/1907.13223.pdf also temporal data for simple ANN with backprop is covered in A hierarchical neural-network model…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/temporal-coding-in-spiking-neural-networks-with-alpha-synaptic-function-learning-with-backpropagation.md","frontmatter":{"title":"Temporal Coding in Spiking Neural Networks","date":null,"topic":null,"article":null},"id":"e778ee8e-4b39-5277-a4d6-bd1b244441a2"},"previous":null,"recentArticles":[{"excerpt":"I am reading about Discovering of useful questions as auxiliary tasks and by discovery they mean using General value function, GVF that is…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/discovery-of-question-by-reinforcement-learning-agent.md","frontmatter":{"title":"Discovery of question","date":null,"topic":null,"article":null},"id":"df048bd6-6bc6-5207-9529-36985ba46322"},{"excerpt":"PDF: https://apps.dtic.mil/sti/pdfs/AD1101848.pdf","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/enabling-brain-inspired-processors-through-energy-efficient-delayed-feedback-reservoir-computing-integrated-circuits.md","frontmatter":{"title":"Enabling Brain-Inspired Processors","date":null,"topic":null,"article":null},"id":"7d04f35f-8982-500e-9ce3-f46e7478b821"},{"excerpt":"PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798047/#!po=48.6842","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/firing-patterns-in-the-adaptive-exponential-integrate-and-fire-model.md","frontmatter":{"title":"Firing patterns in the adaptive exponential integrate-and-fire model","date":null,"topic":null,"article":null},"id":"796a597e-d387-5cba-bc16-b9082a1d4857"},{"excerpt":"Pattern-atom (patom) theory = brain is a pattern matching machine. Seems like very stubborn and contradicting approach. It assumes that…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/patom-theory.md","frontmatter":{"title":"Patom theory","date":null,"topic":null,"article":null},"id":"f79d6108-e0fd-568d-b6e9-e728549fd3e1"},{"excerpt":"First I've seen it on Twitter and I was intrigued by simple and cartoonish environment.\nIt has simple representation to set its main focus…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reviews/reward-is-not-necessary.md","frontmatter":{"title":"Beaver escaping from prison","date":null,"topic":null,"article":null},"id":"3b0c0162-6ff3-5f08-8c2f-40aa9eacbc9a"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}