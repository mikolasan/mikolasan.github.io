{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/active-inference","result":{"data":{"markdownRemark":{"html":"<p>Everyone is talking about active inference but no one implementing it. Right?</p>\n<p>This is what makes me think that everyone is talking about it:</p>\n<ul>\n<li><a href=\"https://putanumonit.com/2023/08/19/seth-explains-consciousness/\" title=\"https://putanumonit.com/2023/08/19/seth-explains-consciousness/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">A book</a>  - <strong>Being You</strong> by Anil Seth</li>\n<li><a href=\"https://m.youtube.com/watch?v=bk_xCikDUDQ\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">ML street talk episode</a> a book \"<strong>Active Inference</strong>: The Free Energy Principle in Mind, Brain and Behavior\" by Thomas Parr</li>\n</ul>\n<h2 id=\"theory\" style=\"position:relative;\">Theory<a href=\"#theory\" aria-label=\"theory permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>One thing is when one says that all our actions heavily use background inference (for more context I will send the reader to Anil Seth’s book), because that is how organisms quickly react (requiring no processing from scratch) or correct noise (where we would need to compare the two images) and more but I want to focus on <em>how</em>.</p>\n<p>But it is definitely an another thing when one looks at standard pyramidal hierarchy of artificial neural networks and tries to figure out where is that inference. The closest architecture variation would be autoencoders. They have a shape of hourglasses: information from sensors passed through layers, features extracted, then these features used in a reverse process of recreating something similar to the input. But I don’t think it would work as inference.</p>\n<h3 id=\"articles\" style=\"position:relative;\">Articles<a href=\"#articles\" aria-label=\"articles permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<ul>\n<li><a href=\"https://www.nature.com/articles/s42003-021-02994-2#Equ6\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Canonical neural networks perform active inference</a></li>\n</ul>\n<h2 id=\"ideas\" style=\"position:relative;\">Ideas<a href=\"#ideas\" aria-label=\"ideas permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p><a href=\"/ai/long-short-term-memory\">LSTM</a> but it compares an input with a full pattern from memory</p>","excerpt":"Everyone is talking about active inference but no one implementing it. Right? This is what makes me think that everyone is talking about it…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#theory\">Theory</a></p>\n<ul>\n<li><a href=\"#articles\">Articles</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#ideas\">Ideas</a></p>\n</li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/08665ed1-0559-52bc-bb9f-1f1beed23815.jpg"},"frontmatter":{"date":"September 04, 2024","published":"September 04, 2024","lastModified":"September 04, 2024","title":"Active inference","subtitle":"in artificial neural networks","section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/active-inference.md","url":"/ai/active-inference","next":{"excerpt":"This theory is about hierarchical structure of neural networks, regardless of whether they are spiking or simple feedforward networks. Every…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/ultra-planar-theory.md","frontmatter":{"title":"Ultra planar theory","date":"2024-09-10T00:00:00.000Z","topic":null,"article":"main"},"id":"abe652e6-53d8-5d20-ab17-9cc1977c4d69"},"previous":{"excerpt":"Maybe you have a job position as a data scientist or ML researcher, but you cannot be called that way if you cannot answer the following…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/true-ml-researcher.md","frontmatter":{"title":"ML researcher interview questions","date":"2024-08-23T00:00:00.000Z","topic":null,"article":"quest"},"id":"2aa7290b-9cf5-5b56-9278-dffb46583623"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}