{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/alternatives-to-gradient-descent","result":{"data":{"markdownRemark":{"html":"<h2 id=\"improvements\" style=\"position:relative;\">Improvements<a href=\"#improvements\" aria-label=\"improvements permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>First let's review improvements for Gradient Descent. They are still GD by its nature.</p>\n<ol start=\"7\">\n<li>Momentum gradient descent (MGD)</li>\n<li>Nesterov accelerated gradient descent (NAG)</li>\n<li>Adaptive gradient (Adagrad)</li>\n<li>Root-mean-square gradient propagation (RMSprop)</li>\n<li>Adaptive moment estimation (Adam)</li>\n</ol>\n<p>Source: from an alternative ANN architecture - <strong>Backpropagation Neural Tree</strong> <a href=\"https://arxiv.org/pdf/2202.02248.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a></p>\n<h2 id=\"alternatives\" style=\"position:relative;\">Alternatives<a href=\"#alternatives\" aria-label=\"alternatives permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>When it comes to <strong>Global Optimisation</strong> tasks (i.e. attempting to find a global minimum of an objective function) you might wanna take a look at:</p>\n<ol>\n<li><a href=\"https://en.wikipedia.org/wiki/Pattern_search_(optimization)\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><em>Pattern Search</em></a> (also known as <em>direct search, derivative-free search</em>, or black-box search), which uses a <em>pattern</em> (set of vectors <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{v_i\\}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">}</span></span></span></span></span></span>) to determine the points to search at next iteration.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><em>Genetic Algorithm</em></a> that uses the concept of mutation, crossover and selection to define the population of points to be evaluated at next iteration of the optimisation.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Particle_swarm_optimization\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><em>Particle Swarm Optimisation</em></a> that defines a set of particles that \"walk\" through the space searching for the minimum.</li>\n<li><a href=\"https://www.mathworks.com/help/gads/surrogate-optimization-algorithm.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><em>Surrogate Optimisation</em></a> that uses a <em>surrogate</em> model to approximate the objective function. This method can be used when the objective function is expensive to evaluate.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Multi-objective_optimization\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Multi-objective Optimisation</a> (also known as <em>Pareto optimisation</em>) which can be used for the problem that cannot be expressed in a form that has a single objective function (but rather a vector of objectives).</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Simulated_annealing\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><em>Simulated Annealing</em></a>, which uses the concept of <em>annealing</em> (or temperature) to trade-off exploration and exploitation. It proposes new points for evaluation at each iteration, but as the number of iteration increases, the \"temperature\" drops and the algorithm becomes less and less likely to explore the space thus \"converging\" towards its current best candidate.</li>\n<li>Backpropagation Neural Tree (BNeuralT)</li>\n<li><a href=\"https://arxiv.org/abs/1412.7525#\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><strong>Target Propagation</strong></a></li>\n<li><a href=\"https://arxiv.org/abs/1605.02026\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><strong>Alternating Descent Method of Multipliers</strong></a> (ADMM) (You need 7000 CPU cores on a supercomputer, while I can have 1280 GPU cores on my old laptop with GeForce GTX 1060)</li>\n<li><a href=\"https://arxiv.org/abs/2011.08895\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><strong>Zeroth-Order Relaxed Backpropagation</strong></a> (ZORB)</li>\n<li>Finito <a href=\"https://arxiv.org/abs/1407.2710\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://arxiv.org/abs/1407.2710</a></li>\n<li>Stochastic Dual Coordinate Ascent (SDCA) <a href=\"https://arxiv.org/abs/1209.1873\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://arxiv.org/abs/1209.1873</a></li>\n<li>Stochastic Optimization with Variance Reduction <a href=\"https://hal.inria.fr/hal-01375816v1/document\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://hal.inria.fr/hal-01375816v1/document</a></li>\n<li>Spike timing-dependent plasticity (STDP) for <a href=\"/ai/spiking-neural-networks\">Spiking Neural Networks</a></li>\n<li>Feedback alignment (FA)</li>\n</ol>\n<p>As mentioned above, <em>Simulated Annealing, Particle Swarm Optimisation and Genetic Algorithms</em> are good global optimisation algorithms that navigate well through huge search spaces and unlike <em>Gradient Descent</em> do not need any information about the gradient and could be successfully used with black-box objective functions and problems that require running simulations.</p>\n<p>Source: <a href=\"https://stats.stackexchange.com/questions/97014/what-are-alternatives-of-gradient-descent\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">1</a> <a href=\"https://stackoverflow.com/questions/23554606/what-are-alternatives-of-gradient-descent\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">2</a></p>\n<h2 id=\"vityaevs-method\" style=\"position:relative;\">Vityaev's method<a href=\"#vityaevs-method\" aria-label=\"vityaevs method permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Vityaev’s method gives more flexibility to the dendrites part of the system comparing to how they are modeled in perceptrons or any deep network, instead of simple weights per synapses it reacts to signal patterns like in spiking neurons. Moreover, using this framework it’s possible to interpret signal propagation with logic statements and probabilities like in Bayes networks. Thus we have a bigger choice of learning methods. It can be some statistical analysis of data.</p>\n<h2 id=\"extreme-learning\" style=\"position:relative;\">Extreme Learning<a href=\"#extreme-learning\" aria-label=\"extreme learning permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://web.njit.edu/~usman/courses/cs675_fall20/ELM-NC-2006.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">official</a> (?)</li>\n<li><a href=\"https://pdfs.semanticscholar.org/13be/dd5a3299a115ecc425eff6e7853741c81816.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">matrix notation</a></li>\n<li><a href=\"https://erdem.pl/2020/05/introduction-to-extreme-learning-machines\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">lame explanation</a></li>\n<li><a href=\"https://github.com/burnpiro/elm-pure/blob/master/model.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">python code</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Extreme_learning_machine\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">wiki</a></li>\n<li><a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&#x26;arnumber=7140733\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a></li>\n</ul>\n<h2 id=\"towards-a-more-biologically-plausible-learning-algorithm\" style=\"position:relative;\">Towards a more biologically plausible learning algorithm<a href=\"#towards-a-more-biologically-plausible-learning-algorithm\" aria-label=\"towards a more biologically plausible learning algorithm permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Main article - Biologically-Plausible Learning Algorithms Can Scale to Large Datasets <a href=\"https://arxiv.org/pdf/1811.03567.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">link</a></p>\n<h2 id=\"nashs-equilibrium\" style=\"position:relative;\">Nash’s equilibrium<a href=\"#nashs-equilibrium\" aria-label=\"nashs equilibrium permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Why is gradient descent the most popular, the only one technique in use? Because it’s stable. No matter what’s in your training data, with any order you will get predictable outcome. And things like Adaptive Resonance Theory aren't stable in that regard. They learn fast, but the order of data is very important. But. What if we apply Nash’s equilibrium here? Every system, component in the brain fight for their truth and shift the system, they play against each other, but we can determine when they must stop overwriting each other. Will it fix the problem with order?</p>","excerpt":"Improvements First let's review improvements for Gradient Descent. They are still GD by its nature. Momentum gradient descent (MGD) Nesterov…","tableOfContents":"<ul>\n<li><a href=\"#improvements\">Improvements</a></li>\n<li><a href=\"#alternatives\">Alternatives</a></li>\n<li><a href=\"#vityaevs-method\">Vityaev's method</a></li>\n<li><a href=\"#extreme-learning\">Extreme Learning</a></li>\n<li><a href=\"#towards-a-more-biologically-plausible-learning-algorithm\">Towards a more biologically plausible learning algorithm</a></li>\n<li><a href=\"#nashs-equilibrium\">Nash’s equilibrium</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/a39e9857-46a4-50e0-90aa-a814c61ddb0c.jpg"},"frontmatter":{"date":"November 13, 2022","published":"January 03, 2023","lastModified":"October 04, 2023","title":"Alternatives to gradient descent","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/alternatives-to-gradient-descent.md","url":"/ai/alternatives-to-gradient-descent","next":{"excerpt":"Neural networks, classification and clusterization algorithms are considered to be intelligent as far as they are part of Artificial…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/markov-decision-processes.md","frontmatter":{"title":"Markov decision processes","date":"2023-01-01T00:00:00.000Z","topic":true,"article":"main"},"id":"bc7cf9ce-5862-558c-8c92-8a90d57c1b3b"},"previous":{"excerpt":"In this post we collect known research about Artificial Neural Networks and their property to approximate non-linear functions and logical…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reinforcement-learning-using-artificial-neural-networks.md","frontmatter":{"title":"Reinforcement Learning using ANN","date":"2022-10-25T00:00:00.000Z","topic":null,"article":"main"},"id":"6e5bdc7e-58d9-578a-8c49-5ee5cd4a543a"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}