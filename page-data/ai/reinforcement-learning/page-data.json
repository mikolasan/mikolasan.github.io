{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/reinforcement-learning","result":{"data":{"markdownRemark":{"html":"<h2 id=\"roadmap\" style=\"position:relative;\">Roadmap<a href=\"#roadmap\" aria-label=\"roadmap permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>Tabular (<a href=\"https://www.cs.cmu.edu/~epxing/Class/10708-20/scribe/lec20_scribe.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Lecture 20</a>, <a href=\"https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Huggingface course</a>, <a href=\"https://mlabonne.github.io/blog/posts/2022-02-13-Q_learning.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">develop the thing from scratch</a>)</li>\n<li>DQN (<a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">DQN Paper</a> and implement something better than <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">any</a> <a href=\"https://github.com/Apress/deep-reinforcement-learning-python/blob/main/chapter6/listing6_1_dqn_pytorch.ipynb\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Python</a> code)</li>\n<li>PPO</li>\n<li>Distributional RL (<a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">good explanation</a>)</li>\n<li>Rainbow (<a href=\"https://github.com/Kaixhin/Rainbow\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">github</a>, <a href=\"https://arxiv.org/abs/1710.02298\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a>)</li>\n<li>Multi goal</li>\n<li>Multi agent</li>\n<li>Continuous time</li>\n<li>SAC</li>\n<li>REINFOCE</li>\n<li>Dreamer</li>\n<li>td3</li>\n<li>Planning (<a href=\"https://github.com/yandexdataschool/Practical_RL\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">a practical course from Yandex</a>)</li>\n</ul>\n<h3 id=\"environments\" style=\"position:relative;\">Environments<a href=\"#environments\" aria-label=\"environments permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<ul>\n<li>OpenAI gym (<a href=\"https://gymnasium.farama.org/environments/classic_control/cart_pole/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Farama</a>)</li>\n<li><a href=\"https://pettingzoo.farama.org/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Petting Zoo</a> - multi-agent</li>\n</ul>\n<p>Sims:</p>\n<ul>\n<li>Isaac Sim</li>\n<li>MuJoCo</li>\n<li>Webots (<a href=\"https://mpolinowski.github.io/docs/Automation_and_Robotics/Robotics_Simulation/2023-11-26--ros2-webots/2023-11-26/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">in ROS2</a>)</li>\n<li>Gazebo</li>\n</ul>\n<p>Walking bipeds:</p>\n<ul>\n<li><a href=\"https://huggingface.co/p3nGu1nZz/Kyle-b0a\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Kyle</a> (<a href=\"https://github.com/cat-game-research/Neko/tree/main/RagdollTrainer\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a> from \"cat crime research\")</li>\n<li>???</li>\n</ul>\n<h2 id=\"personal-research\" style=\"position:relative;\">Personal research<a href=\"#personal-research\" aria-label=\"personal research permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p><a href=\"/ai/reinforcement-learning-using-artificial-neural-networks\">Reinforcement Learning using ANN</a></p>\n<h2 id=\"articles\" style=\"position:relative;\">Articles<a href=\"#articles\" aria-label=\"articles permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2102.07716.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">How RL Agents Behave When Their Actions Are Modified</a>, <a href=\"https://github.com/edlanglois/mamdp\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a></li>\n<li>soccer bi-pedal robot recover after push with Deep Reinforcement Learning <a href=\"https://sites.google.com/view/op3-soccer?pli=10\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">site</a> <a href=\"https://arxiv.org/abs/2304.13653\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a></li>\n<li>2019 Neftci - Reinforcement learning in artificial and biological systems <a href=\"https://www.gwern.net/docs/reinforcement-learning/model-free/2019-neftci.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.gwern.net/docs/reinforcement-learning/model-free/2019-neftci.pdf</a></li>\n<li>neurorobotics can be used to explain how neural network activity leads to behavior. <a href=\"https://www.frontiersin.org/articles/10.3389/fnbot.2020.570308/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Neurorobots as a Means Toward Neuroethology and Explainable AI</a></li>\n<li><a href=\"https://www.tensorflow.org/agents/tutorials/0_intro_rl\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">TensorFlow tutorials</a> probably bad about theory part, but <a href=\"https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Hugging Face course</a> is definetily more clear.</li>\n<li>Explnataion-based learning VS Reinforcement learning <a href=\"https://link.springer.com/article/10.1023/A:1007355226281\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Dietterich 1997</a></li>\n<li>HOT! (According to <a href=\"https://medium.com/@ignacio.de.gregorio.noblejas/offline-rl-680450c472c\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Ignacio de Gregorio</a>, pulp fiction writer: In a paper that’s not even been officially presented yet, Google has announced pre-trained robots that are capable of doing multiple different activities and also be easily trained to ambitious downstream tasks.) <a href=\"https://arxiv.org/pdf/2211.15144.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://arxiv.org/pdf/2211.15144.pdf</a></li>\n<li><a href=\"https://www.nature.com/articles/s42256-023-00701-w\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Review of clutches</a> required in RL</li>\n<li>Smart Reinforcement learning in <a href=\"https://arxiv.org/pdf/2211.10851.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">How to Create a Modular &#x26; Compositional Self-Preserving Agent for Life-Long Learning</a>. It is based on <a href=\"https://medium.com/@ngao7/markov-decision-process-basics-3da5144d3348\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Markov decision processes</a> (too wordy, but very simple articles with examples) that uses Bellman equation, <a href=\"http://incompleteideas.net/book/ebook/node34.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">value functions</a>, <a href=\"https://arxiv.org/pdf/2211.10851.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">temporal goals in MDP</a>. I think one can rewrite <a href=\"https://www.tech-quantum.com/markov-decision-process-implemented-from-scratch/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this smelly python code</a> while using <a href=\"https://github.com/sawcordwell/pymdptoolbox/blob/master/src/mdptoolbox/mdp.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this popular python library</a> (470 stars) as reference.</li>\n<li>Game theory in RF. <a href=\"https://arxiv.org/pdf/2206.15378.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning</a>. Where you need to know what is <a href=\"https://www.pnas.org/doi/full/10.1073/pnas.1400823111\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">The replicator equation</a> and how to plot <a href=\"https://www.biorxiv.org/content/10.1101/300004v2.full.pdf#page91\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">3-Strategy Evolutionary Games</a></li>\n<li><a href=\"https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Policy gradient methods</a> -> <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Proximal policy optimization</a></li>\n</ul>\n<h2 id=\"questions\" style=\"position:relative;\">Questions<a href=\"#questions\" aria-label=\"questions permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>Does anyone know if the reward function in reinforcement learning was ever supported by logical reasoning instead of a priori given values?</li>\n</ul>\n<h2 id=\"program\" style=\"position:relative;\">Program<a href=\"#program\" aria-label=\"program permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I have read <a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">DQN Paper</a> and implement something better than <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">any</a> <a href=\"https://github.com/Apress/deep-reinforcement-learning-python/blob/main/chapter6/listing6_1_dqn_pytorch.ipynb\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Python</a> code.</p>\n<p>Let's write Reinforcement Learning program in Rust. In the beginning we will be solving frost lake environment. We will use a <a href=\"https://github.com/tspooner/rsrl/blob/master/rsrl_domains/src/grid_world.rs\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">grid world</a> from <code class=\"language-text\">rsrl</code></p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\"><span class=\"token function\">cargo</span> new qu-robot\n<span class=\"token builtin class-name\">cd</span> qu-robot\n<span class=\"token function\">cargo</span> <span class=\"token function\">add</span> rsrl</code></pre></div>\n<p>Walls in the grid environment is an interesting problem. In <code class=\"language-text\">rsrl</code> crate it was implemented that the movement is possible but not movement actually occurs, so the agent just stays in the same position. It's not good. Because stay in place should be intentional action rather than a loophole in the implementation. Staying in the same spot is an equilibrium, so we can say it's safe strategy.</p>\n<p>What can we do to fix it? We can remove movements that go into any wall from a list of possible actions. But then the sum of probabilities for remaining actions stops being equal to 1. And this can be solved by normalization, but I have more important vote against it. I think that the agent should understand something about the environment and know what walls are. As thy can be the limits of the environment and also any obstacle inside the environment. And if the agent finds a way to avoid the obstacles then it can apply the same technique to the limits of the environment and leave the box...</p>\n<p>Then if we add another state 'W' everywhere around our current grid then we emphasize the problem that we had originally. Every decision takes in consideration only the current state. If this state was near a wall then we learned that that direction is bad. Instead of just observing what is near our agent is taking actions like a blind hedgehog in the dark. So then we need to combine all cells around into one state.</p>\n<p>But this is just very exponentially increased our set of possible states and dimensions of transition matrix. Then let's only include states that have reward values. That's not every state, right?</p>\n<p>We skip diogonal values. Thus we have 5 states that will create one meta state. Having only 3 meaningful states we have 5^3 = 125 meta states.</p>\n<h2 id=\"further-development\" style=\"position:relative;\">Further development<a href=\"#further-development\" aria-label=\"further development permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<h3 id=\"sub-goals\" style=\"position:relative;\">Sub goals<a href=\"#sub-goals\" aria-label=\"sub goals permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p><strong>Goal-Conditioned Reinforcement Learning: Problems and Solutions</strong> by Liu et al (<a href=\"https://arxiv.org/pdf/2201.08299\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a>, 2022)\nGoals are positions where robot should reach (it's body, or parts like arms). There are many end positions that correspond to different tasks or steps. These positions can be discovered with several methods, so no need  to hard-code them. But there is no single answer how to switch between goals. It resembles very much bayesian learning where conditional probabilities are learned from experience.</p>\n<h3 id=\"nash-equilibrium\" style=\"position:relative;\">Nash equilibrium<a href=\"#nash-equilibrium\" aria-label=\"nash equilibrium permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p><a href=\"https://arxiv.org/pdf/2404.03715.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">RL and nash equlibrium</a></p>\n<h3 id=\"stabilization-tricks\" style=\"position:relative;\">Stabilization tricks<a href=\"#stabilization-tricks\" aria-label=\"stabilization tricks permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>In actor-critic:</p>\n<blockquote>\n<p>To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).</p>\n<p><a href=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic#2_compute_the_expected_returns\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Actor-critic when calculating expected returns</a></p>\n</blockquote>\n<p>For normalization layer (<a href=\"https://discord.com/channels/879548962464493619/915190889243103282/1263736433697362071\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">by Kazumi</a>):</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">mean <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>square<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nsquare <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>square<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\nself<span class=\"token punctuation\">.</span>activation <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>square <span class=\"token operator\">-</span> <span class=\"token number\">2</span><span class=\"token operator\">*</span>torch<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>square<span class=\"token operator\">-</span>mean<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>","excerpt":"Roadmap Tabular (Lecture 20, Huggingface course, develop the thing from scratch) DQN (DQN Paper and implement something better than any…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#roadmap\">Roadmap</a></p>\n<ul>\n<li><a href=\"#environments\">Environments</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#personal-research\">Personal research</a></p>\n</li>\n<li>\n<p><a href=\"#articles\">Articles</a></p>\n</li>\n<li>\n<p><a href=\"#questions\">Questions</a></p>\n</li>\n<li>\n<p><a href=\"#program\">Program</a></p>\n</li>\n<li>\n<p><a href=\"#further-development\">Further development</a></p>\n<ul>\n<li><a href=\"#sub-goals\">Sub goals</a></li>\n<li><a href=\"#nash-equilibrium\">Nash equilibrium</a></li>\n<li><a href=\"#stabilization-tricks\">Stabilization tricks</a></li>\n</ul>\n</li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/df72c525-91b8-57c1-8dcb-a086f877855f.jpg"},"frontmatter":{"date":null,"published":"December 20, 2022","lastModified":"December 20, 2022","title":"Reinforcement Learning","subtitle":null,"section":"brain","draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/reinforcement-learning.md","url":"/ai/reinforcement-learning","next":{"excerpt":"Temporal Coding Radix Encoding","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/neural-coding.md","frontmatter":{"title":"Neural coding","date":null,"topic":true,"article":null},"id":"c159a089-e745-54e1-b9d2-a4c2e2e88253"},"previous":{"excerpt":"ANN should have notion of time. RNN can do it Event sequence prediction How to encode time in ANN? With the help of another ANN that will…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/recurrent-neural-networks.md","frontmatter":{"title":"Recurrent Neural Networks","date":null,"topic":true,"article":null},"id":"ad5048df-ca32-5b3c-a369-5ac2b6e07cf4"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}