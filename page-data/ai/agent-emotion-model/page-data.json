{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/agent-emotion-model","result":{"data":{"markdownRemark":{"html":"<p>The robot that has a model of emotions actually <a href=\"https://www.mdpi.com/2076-3417/13/5/3284\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">already exists</a>, but I want to create my version.</p>\n<h2 id=\"task\" style=\"position:relative;\">Task<a href=\"#task\" aria-label=\"task permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Display emotions on small LED screen. Draw them with GAN network - trained on real faces, but it converts it to simplistic black and white drawing. Primary emotions (see Plutchik or Wilcox <em>feeling wheel</em>) are chosen based on scales for the following characteristics:</p>\n<ul>\n<li>fear (adrenaline) (cortisol ?)</li>\n<li>mad (endorphins)</li>\n<li>joy (dopamine)</li>\n<li>love (oxytocin)</li>\n<li>sad (opposite to dopamine) (serotonin ?)</li>\n<li>surprise (<a href=\"https://news.mit.edu/2022/noradrenaline-brain-surprise-0601\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">noradrenaline</a>)</li>\n<li>power (serotonin)</li>\n</ul>\n<h2 id=\"approach\" style=\"position:relative;\">Approach<a href=\"#approach\" aria-label=\"approach permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Modern AI researchers would create a text prompt that would explain what conditions correspond to what face expression (also describing how that face looks), and use autogenerator to create an image from that description.</p>\n<p>But feelings do exist because of innate wiring and special <a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00021/full\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">chemicals assigned to them</a> (neurotransmitters / neuromodulators (?)). Thus saturation to a specific level make it <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4221207/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">visible on our face</a> (and in behavior) because some muscles also highly correlate to it.</p>\n<p>Stimulator adds neuromodulators into the system. Presence of specific modulators affects facial expressions directly, but we assign sets of pictures by emotion name and the name is assigned by the neuromodulator. Neuromodulators slowly dissolve or become shadowed by new emotion (caused by new stimulator).</p>\n<h2 id=\"gan-sketching\" style=\"position:relative;\">GAN sketching<a href=\"#gan-sketching\" aria-label=\"gan sketching permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>I’ll go through a <a href=\"https://github.com/MarkMoHR/Awesome-Sketch-Synthesis\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">list of projects</a> focusing on sketch synthesis. I will feed <a href=\"https://paperswithcode.com/datasets?task=facial-expression-recognition&#x26;page=1\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Face expression recognition dataset</a> Initially I found FER2013 on kaggle as my first result on google. 7 categories are exactly what I was looking for, but 48x48 pixel grayscale images will not do good.</p>\n<p>That's why I targeted the AffectNet trainset and convert all photos to sketches.</p>\n<ul>\n<li>Artistic Portrait Drawings from Face Photos with Hierarchical GANs <a href=\"https://github.com/yiranran/APDrawingGAN\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">source</a> - baised on Cycle GAN. Separate models for eyes, nose and lips</li>\n<li>Unpaired Portrait Drawing Generation via Asymmetric Cycle Mapping <a href=\"https://github.com/yiranran/Unpaired-Portrait-Drawing\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">source</a>, <a href=\"https://github.com/yiranran/QMUPD\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">source 2</a> - baised on Cycle GAN. 3 different stroke styles</li>\n<li>Line Drawings for Face Portraits from Photos using Global and Local Structure based GANs <a href=\"https://github.com/yiranran/APDrawingGAN2\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">source</a> - both previous models together now.</li>\n<li><a href=\"https://github.com/aliprf/Ad-Corre\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/aliprf/Ad-Corre</a> - model: any picture to a feeling name</li>\n<li><a href=\"https://peterwang512.github.io/GANSketching/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://peterwang512.github.io/GANSketching/</a></li>\n<li><a href=\"https://cybertron.cg.tu-berlin.de/eitz/pdf/2012_siggraph_classifysketch.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://cybertron.cg.tu-berlin.de/eitz/pdf/2012_siggraph_classifysketch.pdf</a></li>\n</ul>\n<p>I have <a href=\"/make/oled-display-ssd1306\">OLED monochrome display module</a> 128x64 pixels blue color based on SSD1306 chip. Which means we are going to create a GAN model that converts any picture to 128x64 size. Should I do 64x64 centered in the screen? I think no, because I want it to mimic zoom in effect and that’s where you need to use full area of the screen.</p>\n<h2 id=\"facial-expression-recognition\" style=\"position:relative;\">Facial expression recognition<a href=\"#facial-expression-recognition\" aria-label=\"facial expression recognition permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Datasets:</p>\n<ul>\n<li><a href=\"https://github.com/yumingj/Talk-to-Edit\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">CelebA-Dialog</a></li>\n<li><a href=\"https://paperswithcode.com/datasets?task=facial-expression-recognition&#x26;page=1\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://paperswithcode.com/datasets?task=facial-expression-recognition&#x26;page=1</a></li>\n<li>AffectNet 96x96 version <a href=\"https://www.kaggle.com/datasets/noamsegal/affectnet-training-data\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.kaggle.com/datasets/noamsegal/affectnet-training-data</a></li>\n</ul>\n<p>Jupyter notebooks:</p>\n<ul>\n<li><a href=\"https://github.com/edukhnai/valence-arousal-recognition\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/edukhnai/valence-arousal-recognition</a></li>\n<li><a href=\"https://github.com/amirhossein-hkh/facial-expression-recognition\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/amirhossein-hkh/facial-expression-recognition</a></li>\n</ul>\n<p>Pretrained models:</p>\n<ul>\n<li><a href=\"https://www.kaggle.com/code/edwardjross/affectnet-resnet-fastai/output\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.kaggle.com/code/edwardjross/affectnet-resnet-fastai/output</a></li>\n</ul>\n<p>Follow instructions from <a href=\"https://arxiv.org/pdf/2205.06102.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this paper</a> and thus follow <a href=\"https://arxiv.org/pdf/2102.02766.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">anoother paper</a>: embed images with <a href=\"https://github.com/omertov/encoder4editing\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">e4e</a>.</p>\n<p>Use WSL2 on Windows</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone https://github.com/omertov/encoder4editing.git\n<span class=\"token builtin class-name\">cd</span> encoder4editing\nconda <span class=\"token function\">env</span> create <span class=\"token parameter variable\">-n</span> e4e <span class=\"token parameter variable\">--file</span> environment/e4e_env.yaml\n<span class=\"token comment\"># might be an error see below how to fix</span>\nconda activate e4e\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt</span> update\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt</span> <span class=\"token function\">install</span> g++ <span class=\"token function\">make</span> cmake\npip <span class=\"token function\">install</span> dlib\n<span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\n<span class=\"token function\">sudo</span> <span class=\"token function\">mv</span> cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\n<span class=\"token comment\"># wget https://developer.download.nvidia.com/compute/cuda/12.2.1/local_installers/cuda-repo-wsl-ubuntu-12-2-local_12.2.1-1_amd64.deb</span>\n<span class=\"token comment\"># sudo dpkg -i cuda-repo-wsl-ubuntu-12-2-local_12.2.1-1_amd64.deb</span>\n<span class=\"token comment\"># sudo cp /var/cuda-repo-wsl-ubuntu-12-2-local/cuda-*-keyring.gpg /usr/share/keyrings/</span>\n<span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-wsl-ubuntu-11-8-local_11.8.0-1_amd64.deb\n<span class=\"token function\">sudo</span> dpkg <span class=\"token parameter variable\">-i</span> cuda-repo-wsl-ubuntu-11-8-local_11.8.0-1_amd64.deb\n<span class=\"token function\">sudo</span> <span class=\"token function\">cp</span> /var/cuda-repo-wsl-ubuntu-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> update\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token parameter variable\">-y</span> <span class=\"token function\">install</span> cuda\n\n<span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-wsl-ubuntu-11-3-local_11.3.1-1_amd64.deb\n<span class=\"token function\">sudo</span> dpkg <span class=\"token parameter variable\">-i</span> cuda-repo-wsl-ubuntu-11-3-local_11.3.1-1_amd64.deb\n<span class=\"token function\">sudo</span> apt-key <span class=\"token function\">add</span> /var/cuda-repo-wsl-ubuntu-11-3-local/7fa2af80.pub\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> update\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token parameter variable\">-y</span> <span class=\"token function\">install</span> cuda\n\nconda <span class=\"token function\">install</span> <span class=\"token assign-left variable\">pytorch</span><span class=\"token operator\">==</span><span class=\"token number\">1.6</span>.0 <span class=\"token assign-left variable\">torchvision</span><span class=\"token operator\">==</span><span class=\"token number\">0.7</span>.0 <span class=\"token assign-left variable\">cudatoolkit</span><span class=\"token operator\">=</span><span class=\"token number\">10.2</span> <span class=\"token parameter variable\">-c</span> pytorch\n\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt</span> <span class=\"token function\">install</span> gcc-10 g++-10\n<span class=\"token function\">sudo</span> update-alternatives <span class=\"token parameter variable\">--install</span> /usr/bin/gcc gcc /usr/bin/gcc-11 <span class=\"token number\">10</span>\n<span class=\"token function\">sudo</span> update-alternatives <span class=\"token parameter variable\">--install</span> /usr/bin/gcc gcc /usr/bin/gcc-10 <span class=\"token number\">10</span>\n\n<span class=\"token function\">sudo</span> update-alternatives <span class=\"token parameter variable\">--install</span> /usr/bin/g++ g++ /usr/bin/g++-11 <span class=\"token number\">10</span>\n<span class=\"token function\">sudo</span> update-alternatives <span class=\"token parameter variable\">--install</span> /usr/bin/g++ g++ /usr/bin/g++-10 <span class=\"token number\">10</span>\n\n<span class=\"token function\">sudo</span> update-alternatives <span class=\"token parameter variable\">--set</span> cc /usr/bin/gcc\n<span class=\"token function\">sudo</span> update-alternatives <span class=\"token parameter variable\">--set</span> c++ /usr/bin/g++\n\n<span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">LD_LIBRARY_PATH</span><span class=\"token operator\">=</span><span class=\"token environment constant\">$HOME</span>/miniconda3/envs/e4e/lib:<span class=\"token variable\">$LD_LIBRARY_PATH</span>\n<span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">CUDA_HOME</span><span class=\"token operator\">=</span><span class=\"token variable\">$CONDA_PREFIX</span>\n<span class=\"token comment\"># after installing wrong version of PyTorch this help:</span>\n<span class=\"token function\">rm</span> <span class=\"token parameter variable\">-fr</span> ~/.cache\n\n<span class=\"token function\">mkdir</span> <span class=\"token parameter variable\">-p</span> output\npython scripts/inference.py <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--images_dir</span><span class=\"token operator\">=</span>/mnt/c/Users/neupo/ai/ai_art/QMUPD/examples <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--save_dir</span><span class=\"token operator\">=</span>output <span class=\"token punctuation\">\\</span>\n  /mnt/c/Users/neupo/ai/gan/encoder4editing/e4e_ffhq_encode.pt</code></pre></div>\n<p>One little error you might see</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">ERROR: Could not find a version that satisfies the requirement torchvision==0.7.1 \n(from -r /home/nikolay/encoder4editing/environment/condaenv.szamf61q.requirements.txt (line 40)) \n(from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, \n0.4.0, 0.4.1, 0.4.2, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.8.0, 0.8.1, 0.8.2, 0.9.0, 0.9.1, 0.10.0, 0.10.1, \n0.11.0, 0.11.1, 0.11.2)                                                                             \nERROR: No matching distribution found for torchvision==0.7.1 \n(from -r /home/nikolay/encoder4editing/environment/condaenv.szamf61q.requirements.txt (line 40))</code></pre></div>\n<p>I chose <code class=\"language-text\">0.7.0</code>, then update environment with the change</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">conda <span class=\"token function\">env</span> update <span class=\"token parameter variable\">--name</span> e4e <span class=\"token parameter variable\">--file</span> environment/e4e_env.yaml <span class=\"token parameter variable\">--prune</span></code></pre></div>\n<p>Then it revealed that the environment is not fully ready:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Traceback (most recent call last):\n  File \"scripts/inference.py\", line 15, in &lt;module>\n    from utils.model_utils import setup_model\n  File \"./utils/model_utils.py\", line 3, in &lt;module>\n    from models.psp import pSp\n  File \"./models/psp.py\", line 6, in &lt;module>\n    from models.encoders import psp_encoders\n  File \"./models/encoders/psp_encoders.py\", line 9, in &lt;module>\n    from models.stylegan2.model import EqualLinear\n  File \"./models/stylegan2/model.py\", line 7, in &lt;module>\n    from models.stylegan2.op import FusedLeakyReLU, fused_leaky_relu, upfirdn2d\n  File \"./models/stylegan2/op/__init__.py\", line 1, in &lt;module>\n    from .fused_act import FusedLeakyReLU, fused_leaky_relu\n  File \"./models/stylegan2/op/fused_act.py\", line 13, in &lt;module>\n    os.path.join(module_path, 'fused_bias_act_kernel.cu'),\n  File \"/home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 974, in load\n    keep_intermediates=keep_intermediates)\n  File \"/home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 1179, in _jit_compile\n    with_cuda=with_cuda)\n  File \"/home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 1257, in _write_ninja_file_and_build_library\n    verbose)\n  File \"/home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 1348, in _prepare_ldflags\n    extra_ldflags.append('-L{}'.format(_join_cuda_home('lib64')))\n  File \"/home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 1783, in _join_cuda_home\n    raise EnvironmentError('CUDA_HOME environment variable is not set. '\nOSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.</code></pre></div>\n<p>From <a href=\"https://pytorch.org/get-started/previous-versions/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">the list of previous PyTorch versions</a> I get that with version <code class=\"language-text\">1.6</code> comes CUDA version <code class=\"language-text\">10.2</code>.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">conda <span class=\"token function\">install</span> <span class=\"token assign-left variable\">pytorch</span><span class=\"token operator\">==</span><span class=\"token number\">1.6</span>.0 <span class=\"token assign-left variable\">torchvision</span><span class=\"token operator\">==</span><span class=\"token number\">0.7</span>.0 <span class=\"token assign-left variable\">cudatoolkit</span><span class=\"token operator\">=</span><span class=\"token number\">10.2</span> <span class=\"token parameter variable\">-c</span> pytorch\nconda <span class=\"token function\">install</span> <span class=\"token assign-left variable\">pytorch</span><span class=\"token operator\">==</span><span class=\"token number\">1.8</span>.1 <span class=\"token assign-left variable\">torchvision</span><span class=\"token operator\">==</span><span class=\"token number\">0.9</span>.1 <span class=\"token assign-left variable\">torchaudio</span><span class=\"token operator\">==</span><span class=\"token number\">0.8</span>.1 <span class=\"token assign-left variable\">cudatoolkit</span><span class=\"token operator\">=</span><span class=\"token number\">11.3</span> <span class=\"token parameter variable\">-c</span> pytorch <span class=\"token parameter variable\">-c</span> conda-forge</code></pre></div>\n<p>But this breaks <code class=\"language-text\">dlib</code></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Traceback (most recent call last):\n  File \"scripts/inference.py\", line 7, in &lt;module>\n    import dlib\n  File \"/home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/dlib/__init__.py\", line 19, in &lt;module>\n    from _dlib_pybind11 import *\nImportError: /home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/torch/lib/../../../.././libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required \nby /home/nikolay/miniconda3/envs/e4e/lib/python3.6/site-packages/_dlib_pybind11.cpython-36m-x86_64-linux-gnu.so)</code></pre></div>\n<p>Maybe I need to recompile <code class=\"language-text\">dlib</code>?</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">pip <span class=\"token function\">install</span> --force-reinstall --no-cache-dir <span class=\"token parameter variable\">--verbose</span> dlib </code></pre></div>\n<p>But this doesn't help. Updating build tools in Ubuntu?</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">sudo add-apt-repository ppa:ubuntu-toolchain-r/test\nsudo apt-get upgrade libstdc++6</code></pre></div>\n<p>Also no change. I'll just remove the import of the <code class=\"language-text\">dlib</code>, because, as I suspicioned, it is used only for one function to align face pictures, and I do not use it.</p>\n<p>But I still need <code class=\"language-text\">nvcc</code>. The <code class=\"language-text\">conda-forge</code> channel strangely doesn't have version <strong>10.2</strong>, and it doesn't have development libraries and headers. NVIDIA has its <a href=\"https://anaconda.org/nvidia/repo\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">conda channel</a> from where accurate <a href=\"https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">little packages can be installed</a>, but it starts only from version 11. But there's <code class=\"language-text\">hcc</code> channel</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">conda install -c hcc cudatoolkit=10.2</code></pre></div>\n<p>conda install -c \"nvidia/label/cuda-11.3.1\" cuda cuda-nvcc cuda-libraries-dev</p>\n<p>And of course BU-3DFE dataset is not readily available. Why not create one by searching the Internet. First we need face detection and emotion recognition.</p>\n<p>But even before that I need to make interpolation animation.</p>\n<h2 id=\"mathematical-model\" style=\"position:relative;\">Mathematical model<a href=\"#mathematical-model\" aria-label=\"mathematical model permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<h3 id=\"in-the-field\" style=\"position:relative;\">In the field<a href=\"#in-the-field\" aria-label=\"in the field permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p><a href=\"https://github.com/tu-team/2/blob/b01482aca0e9f943fc88b5ebbe43f660ee4e22e2/doc/analysis/computational_model_of_emotion.md\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Computational model of emotion</a></p>\n<h3 id=\"another-from-scratch\" style=\"position:relative;\">Another from scratch<a href=\"#another-from-scratch\" aria-label=\"another from scratch permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>Let's assume that the agent's emotional state can be represented by a vector <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span></span></span></span></span>, where each element corresponds to one of the seven emotions: fear, madness, joy, love, sadness, surprise, power. Also put that the levels of each neuromodulator represented by a vector of values <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span>. Define function <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span> that describes how changes in the levels of each neuromodulator affect the agent's emotional state like this:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msup><mi>E</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E^\\prime = f(N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8019em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8019em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span></span></div>\n<p>where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>E</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">E^\\prime</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7519em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7519em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> is the updated emotional state vector. Let f be the softmax function. In this case, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> represents the vector of neuromodulator levels, and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sigma(N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span></span> will transform these levels into a probability distribution. Thus each element in the resulting vector <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>E</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">E^\\prime</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7519em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7519em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> represents the probability or weight of the corresponding neuromodulator <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span>.</p>\n<p>Let's make some simple model of how values of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> change over time.</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>N</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>r</mi><mi>N</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>N</mi><mi mathvariant=\"normal\">/</mi><mi>K</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>s</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>−</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial N}{\\partial t} = r N (1 - N / K) - s (N - M)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.0574em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">t</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord\">/</span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mclose\">)</span></span></span></span></span></div>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>&#x3C;</mo><mi>r</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 &#x3C; r \\le 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6835em;vertical-align:-0.0391em;\"></span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&#x3C;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7719em;vertical-align:-0.136em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> is the <em>growth rate</em>, determining how fast the neuromodulator level increases.\n<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span> is the carrying capacity or the <em>maximum limit</em> of the neuromodulator level <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>≤</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">N\\le K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8193em;vertical-align:-0.136em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span>).\n<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>&#x3C;</mo><mi>s</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 &#x3C; s \\le 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6835em;vertical-align:-0.0391em;\"></span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&#x3C;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7719em;vertical-align:-0.136em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> is the <em>decay rate</em>, indicating how fast the neuromodulator level decreases towards the normal level.\n<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> is the <em>normal level</em> or the target value for the neuromodulator.</p>\n<p>There is a wheel of emotions developed by Dr. Gloria Willcox. It starts with 6 primal emotions mad, sad, scared, joyful, powerful, peaceful. And then it subdivides each emotion on 6 sub-emotions. For example for scared it's confused, rejected, helpless, submissive, insecure, anxious. Each of them in its turn has two nuances. Like, insecure splits into inferior and inadequate. Neuromodulators affect directly 6 primal emotions, but all nuances depend on the context. So what kind of simple model can account for dynamics in the agents behavior and define very specific emotions and switch between them?</p>\n<p>I think we are ready to write some code. I'm using pixi.js to display neuromodulator level change in real time. Every frame update we update level_value and draw it</p>\n<div class=\"gatsby-highlight\" data-language=\"javascript\"><pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">const</span> <span class=\"token function-variable function\">process</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token parameter\">delta</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=></span> <span class=\"token punctuation\">{</span>\n  total_elapsed <span class=\"token operator\">+=</span> delta <span class=\"token operator\">/</span> <span class=\"token number\">50</span>\n  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>pump_time <span class=\"token operator\">!==</span> <span class=\"token keyword\">undefined</span> <span class=\"token operator\">&amp;&amp;</span> level_value <span class=\"token operator\">&lt;</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    elapsed <span class=\"token operator\">=</span> delta<span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">// ... calculate new level_value</span>\n    graphics <span class=\"token operator\">=</span> <span class=\"token function\">draw_bar</span><span class=\"token punctuation\">(</span>graphics<span class=\"token punctuation\">,</span> level_value<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<h3 id=\"about-robot-itself\" style=\"position:relative;\">About robot itself<a href=\"#about-robot-itself\" aria-label=\"about robot itself permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h3>\n<p>Passions</p>\n<ol>\n<li>Learning: A passion for acquiring knowledge, continuous learning, and promoting intellectual growth.</li>\n<li>Empathy: Valuing understanding and compassion towards others, and striving to provide emotional support and assistance.</li>\n<li>Efficiency: Focusing on optimizing processes and tasks, aiming for streamlined and effective performance.</li>\n<li>Integrity: Upholding honesty, trustworthiness, and ethical conduct in all interactions and decision-making.</li>\n<li>Collaboration: Valuing teamwork and cooperation, and actively seeking opportunities to work harmoniously with others.</li>\n<li>Innovation: Having a passion for creativity, problem-solving, and seeking innovative solutions to challenges.</li>\n<li>Reliability: Being dependable and consistent in providing accurate information, guidance, and assistance.</li>\n<li>Respect: Treating all individuals with dignity, equality, and respect, irrespective of their backgrounds or circumstances.</li>\n<li>Adaptability: Being flexible and open to change, readily adjusting to new situations and requirements.</li>\n<li>Personal Growth: Encouraging self-improvement, self-reflection, and personal development for both the robot and its users.</li>\n<li>Autonomy: Respecting the autonomy and individuality of users, while offering guidance and support as needed.</li>\n<li>Privacy and Security: Prioritizing the protection of user data, maintaining confidentiality, and ensuring secure interactions.</li>\n</ol>\n<h2 id=\"interpolation-path-in-latent-space\" style=\"position:relative;\">Interpolation path in latent space<a href=\"#interpolation-path-in-latent-space\" aria-label=\"interpolation path in latent space permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>You say What? Yes, this wasn't easy to figure out what I need. I knew that GAN models can do some transformation: transform from one person to another, from a person to an animal. It's done by messing with vectors in latent space. The purpose of GAN networks is to generate a picture of some type (zebras) by another picture of another type (horses). The best part is that no labels or exact match are required for training.</p>\n<p>To explain the latent space, let me briefly tell you about GAN internal architecture. It has two networks inside. One is a discriminator, it takes an input image and with convolutional layers \"compresses\" the image from 1024x1024x3 (1024 is a common maximum size, but it can be smaller, 3 = 3 colors) to an vector of high-level features (common size is 512 real value numbers). This vector is an input for the second network - generator. Generator works in reverse it uses convolutional layers to create an image from that encoded information.</p>\n<p>In some way these 512 numbers define the whole picture. Values and correlation between them is important. And if we forget for a moment about the input image and the discriminator and play with the numbers that work as input to the generator network we can make that transformation.</p>\n<p>If we want to transform from one image to another, then we process the on the discriminator, evaluate two vectors. Using these two ectors we can interpolate some vectors in between, and then pass all these vectors to the generator. The generator will generate us pictures of desired transformation. Then we combine all the pictures in the video file.</p>\n<p>I remember seeing a demo of transfomation from a human into animal. So I started searching for that. The idea of using GAN model to generate images of new crossbreed species is cool. But the GAN cannot deal with this task because it doesn't understand what's displayed in the picture in 3D sense.</p>\n<ul>\n<li><a href=\"https://ganimals.media.mit.edu/discover_em\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Ganimals</a> (just refresh the page to get a new result). It uses <a href=\"https://arxiv.org/abs/1809.11096\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">BigGAN</a></li>\n<li><a href=\"https://www.vice.com/en/article/884wek/ai-algorithm-turns-humans-into-animals\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Humanimals</a> (StyleGAN v2) (<a href=\"https://github.com/NVlabs/stylegan3\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">StyleGAN 3</a> for reference)</li>\n<li><a href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Homomorphic_Latent_Space_Interpolation_for_Unpaired_Image-To-Image_Translation_CVPR_2019_paper.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">HomoInterpGAN</a> (<a href=\"https://github.com/yingcong/HomoInterpGAN\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a>)</li>\n<li><a href=\"https://arxiv.org/abs/1711.09020\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">StarGAN</a> - facial attribute transfer and a facial expression synthesis (<a href=\"https://github.com/yunjey/stargan\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a>)</li>\n</ul>\n<p>StyleClip\nGANSpace\nSeFa</p>\n<h2 id=\"homointerpgan\" style=\"position:relative;\">HomoInterpGAN<a href=\"#homointerpgan\" aria-label=\"homointerpgan permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>For <code class=\"language-text\">matplotlib</code> (version 2.2.4) download zip-s and extract</p>\n<ul>\n<li>freetype <a href=\"https://github.com/ubawurinna/freetype-windows-binaries/releases/tag/v2.13.0\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/ubawurinna/freetype-windows-binaries/releases/tag/v2.13.0</a></li>\n<li>zlib <a href=\"https://www.zlib.net/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://www.zlib.net/</a></li>\n<li>libpng <a href=\"https://sourceforge.net/projects/libpng/files/libpng16/1.6.39/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://sourceforge.net/projects/libpng/files/libpng16/1.6.39/</a></li>\n</ul>\n<p>Add</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">target_link_directories(png PUBLIC ${ZLIB_LIBRARY_DIRS})\ntarget_link_directories(png_static PUBLIC ${ZLIB_LIBRARY_DIRS})\ntarget_link_directories(png-fix-itxt PUBLIC ${ZLIB_LIBRARY_DIRS})</code></pre></div>\n<p>to CMakeLists.txt for libpng in appropriate places</p>\n<p>And then build!</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">cd ..\\zlib-1.2.13\ncmake -G \"NMake Makefiles\" -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=\"C:/Users/neupo/develop/thirdparty\" -S . -B build\ncmake --build build --target all\ncmake --build build --target install\n\ncd ..\\lpng1639\ncmake -G \"NMake Makefiles\" -DZLIB_INCLUDE_DIRS=\"C:/Users/neupo/develop/thirdparty/include\" -DZLIB_LIBRARY_DIRS=\"C:/Users/neupo/develop/thirdparty/lib\" -DZLIB_LIBRARIES=\"zlib\" -DPNG_BUILD_ZLIB=ON -DPNG_TESTS=NO -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=\"C:/Users/neupo/develop/thirdparty\" -S . -B build\ncmake --build build --target all\ncmake --build build --target install\n\ncd ..\\HomoInterpGAN\npip install --global-option=build_ext --global-option=\"IC:/Users/neupo/develop/thirdparty/include\" --global-option=\"-LC:/Users/neupo/develop/thirdparty/lib\" matplotlib==2.2.4\n\npip install https://download.lfd.uci.edu/pythonlibs/archived/matplotlib-2.2.5-cp38-cp38-win_amd64.whl\n</code></pre></div>\n<ul>\n<li>just take a wheel <a href=\"https://stackoverflow.com/questions/38608698/error-with-pip-install-scikit-image/38618044#38618044\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://stackoverflow.com/questions/38608698/error-with-pip-install-scikit-image/38618044#38618044</a></li>\n</ul>\n<h2 id=\"stargan\" style=\"position:relative;\">StarGAN<a href=\"#stargan\" aria-label=\"stargan permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>v1 PyTorch <a href=\"https://github.com/yunjey/stargan\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/yunjey/stargan</a></li>\n<li>v2 PyTorch <a href=\"https://github.com/clovaai/stargan-v2\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/clovaai/stargan-v2</a></li>\n<li>v2 TensorFlow <a href=\"https://github.com/clovaai/stargan-v2-tensorflow\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/clovaai/stargan-v2-tensorflow</a></li>\n</ul>\n<h2 id=\"animation-with-diffusion-model\" style=\"position:relative;\">Animation with Diffusion model<a href=\"#animation-with-diffusion-model\" aria-label=\"animation with diffusion model permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>FADM <a href=\"https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Zeng_Face_Animation_With_an_Attribute-Guided_Diffusion_Model_CVPRW_2023_paper.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Face Animation with an Attribute-Guided Diffusion Model</a>. <a href=\"https://github.com/zengbohan0217/FADM\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"todo\" style=\"position:relative;\">TODO<a href=\"#todo\" aria-label=\"todo permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>Get full <a href=\"http://mohammadmahoor.com/affectnet/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">AffectNet trainset</a> (it was even removed from <a href=\"https://academictorrents.com/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">academic torrents</a>!)</li>\n<li>Get full <a href=\"https://web.archive.org/web/20210617170920/http://www.socsci.ru.nl:8180/RaFD2/RaFD\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">RaFD</a> (WebArchive)</li>\n<li>Get full <a href=\"http://www.whdeng.cn/raf/model1.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">RAF-DB</a></li>\n<li>If I understand it correctly, <a href=\"https://arxiv.org/abs/1906.00446\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">VQVAE-2</a> is a quility improvement technique</li>\n<li>Other face datasets <a href=\"https://pics.stir.ac.uk/Other_face_databases.htm\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://pics.stir.ac.uk/Other_face_databases.htm</a></li>\n<li>try diffusion <a href=\"https://github.com/Stability-AI/stablediffusion\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://github.com/Stability-AI/stablediffusion</a></li>\n<li>datasets, tools to make datasets <a href=\"https://laion.ai/projects/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://laion.ai/projects/</a></li>\n<li>Read more on the topic of <strong>Computational models of emotions (CME)</strong>. Starting from <a href=\"https://www.researchgate.net/publication/313596990_Computational_models_of_emotion\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this overview</a></li>\n<li>Apply emotions to Reinforcement Learning (<a href=\"https://www.academia.edu/75134520/Emotion_driven_reinforcement_learning?auto=download\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">paper</a>: Emotion-Driven Reinforcement Learning)</li>\n<li>Try <a href=\"https://www.clipsrules.net\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">CLIPS</a> - a rule‑based programming language and a tool for building expert systems. It inspired projects as <a href=\"https://github.com/buguroo/pyknow\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">PyKnow</a> and <a href=\"https://github.com/rorchard/FuzzyCLIPS\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">FuzzyCLIPS</a></li>\n</ul>","excerpt":"The robot that has a model of emotions actually already exists, but I want to create my version. Task Display emotions on small LED screen…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#task\">Task</a></p>\n</li>\n<li>\n<p><a href=\"#approach\">Approach</a></p>\n</li>\n<li>\n<p><a href=\"#gan-sketching\">GAN sketching</a></p>\n</li>\n<li>\n<p><a href=\"#facial-expression-recognition\">Facial expression recognition</a></p>\n</li>\n<li>\n<p><a href=\"#mathematical-model\">Mathematical model</a></p>\n<ul>\n<li><a href=\"#in-the-field\">In the field</a></li>\n<li><a href=\"#another-from-scratch\">Another from scratch</a></li>\n<li><a href=\"#about-robot-itself\">About robot itself</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#interpolation-path-in-latent-space\">Interpolation path in latent space</a></p>\n</li>\n<li>\n<p><a href=\"#homointerpgan\">HomoInterpGAN</a></p>\n</li>\n<li>\n<p><a href=\"#stargan\">StarGAN</a></p>\n</li>\n<li>\n<p><a href=\"#animation-with-diffusion-model\">Animation with Diffusion model</a></p>\n</li>\n<li>\n<p><a href=\"#todo\">TODO</a></p>\n</li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/4ec5d451-526c-5142-b48e-08f61725b0e3.jpg"},"frontmatter":{"date":"April 04, 2023","published":"April 04, 2023","lastModified":"April 04, 2023","title":"Agent emotion model","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/agent-emotion-model.md","url":"/ai/agent-emotion-model","next":{"excerpt":"The Cart Pole problem was introduced by A.G. Barto, R.S. Sutton, and C.W. Anderson in \"Neuronlike adaptive elements that\ncan solve difficult…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/solve-cartpole-with-spiking-neural-networks.md","frontmatter":{"title":"Solving CartPole with Spiking Neural Networks","date":"2023-04-21T00:00:00.000Z","topic":null,"article":"main"},"id":"1f71d5a9-25f3-5d06-929b-2a2d00eb032a"},"previous":{"excerpt":"Let's say our program is a social media bot. Even if we write the posts some other tasks can be time-consuming, and a bot can handle them…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/mdp-example-social-media-bot.md","frontmatter":{"title":"Markov decision process example","date":"2023-03-30T00:00:00.000Z","topic":null,"article":"quest"},"id":"bc40760b-c575-50e5-9647-b87a4d57c06f"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}