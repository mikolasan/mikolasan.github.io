{"componentChunkName":"component---src-templates-blog-template-js","path":"/ai/the-smallest-transformer","result":{"data":{"markdownRemark":{"html":"<h2 id=\"questions\" style=\"position:relative;\">Questions<a href=\"#questions\" aria-label=\"questions permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>Are embeddings required for any type of data? <a href=\"/science/as-always-no-explanation\">Why sin/cos?</a></li>\n<li>What stacks add to the whole picture? How are they different?</li>\n<li>Why 2 parts are called encoder and encoder if they both take input data similarly and process data with almost exact layers? (Only if the first layer of the decoder consisting of the masked multi-head attention is converting encoder into decoder. Or because data from encoder is forwarded into the decoder?). I think that names of these two parts should reflect time-relative context.</li>\n</ul>\n<blockquote>\n<p>The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.</p>\n<p><em><a href=\"https://arxiv.org/abs/1409.0473\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Neural Machine Translation by Jointly Learning to Align and Translate</a></em></p>\n</blockquote>\n<p>Based on <a href=\"https://www.tensorflow.org/text/tutorials/transformer\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">this tutorial</a></p>\n<p>I am trying to understand the difference in training data between sequence2sequence and bidirectional Transformers (like <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">BERT</a>, <a href=\"https://github.com/tensorflow/models/blob/master/official/legacy/bert/bert_models.py\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a>)</p>\n<h2 id=\"reference\" style=\"position:relative;\">Reference<a href=\"#reference\" aria-label=\"reference permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>White papers that we are going to use</p>\n<ul>\n<li><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Original Transformer</a> with code</li>\n<li><a href=\"https://jalammar.github.io/illustrated-transformer/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Illustrated Transformer</a></li>\n<li>The Annotated Transformer <a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">version 1</a> <a href=\"https://nlp.seas.harvard.edu/annotated-transformer/\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">version 2</a> (PyTorch)</li>\n<li><a href=\"https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Embedding</a> is just a way of converting sentences into vectors. More in my notes <a href=\"/science/as-always-no-explanation\">about embeddings</a>.</li>\n<li>Multi-head attention is implemented in TensorFlow: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">docs</a>, <a href=\"https://github.com/tensorflow/addons/blob/v0.20.0/tensorflow_addons/layers/multihead_attention.py#L22-L298\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">code</a>. I checked in code how that <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Layer\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">layer</a> includes separate <a href=\"https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">weights</a> for query, value, and key. For multiplication of tensors they use <a href=\"https://rockt.github.io/2018/04/30/einsum\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">Einstein summation</a> - <strong>einsum</strong> (three dots, ellipsis, to support any higher dimensions).</li>\n</ul>\n<h2 id=\"input-data-and-goal\" style=\"position:relative;\">Input data and goal<a href=\"#input-data-and-goal\" aria-label=\"input data and goal permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Let's write a minimal Transformer model implementation in Rust writing feedforward layers and backpropagation, and decoder-encoder and multi-head attention from scratch. For training we will use a simple sequence <code class=\"language-text\">1 0 1 0 1 0...</code>.</p>\n<h2 id=\"code\" style=\"position:relative;\">Code<a href=\"#code\" aria-label=\"code permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>So let's start coding our simple version. In its core the transformer has encoder-decoder structure.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">as</span> F\n\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Transformer</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>Transformer<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>embedding <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>encoder <span class=\"token operator\">=</span> Encoder<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>decoder <span class=\"token operator\">=</span> Decoder<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>output_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> src<span class=\"token punctuation\">,</span> trg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        src <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>embedding<span class=\"token punctuation\">(</span>src<span class=\"token punctuation\">)</span>\n        trg <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>embedding<span class=\"token punctuation\">(</span>trg<span class=\"token punctuation\">)</span>\n        enc_output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>encoder<span class=\"token punctuation\">(</span>src<span class=\"token punctuation\">)</span>\n        dec_output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>decoder<span class=\"token punctuation\">(</span>trg<span class=\"token punctuation\">,</span> enc_output<span class=\"token punctuation\">)</span>\n        output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>output_layer<span class=\"token punctuation\">(</span>dec_output<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> output</code></pre></div>\n<p>We start with the Encoder which is</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Encoder</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>Encoder<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># use nn.ModuleList instead of nn.Sequential because `num_layers` is an input parameter</span>\n        self<span class=\"token punctuation\">.</span>layers <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ModuleList<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n            EncoderLayer<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_layers<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> layer <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">:</span>\n            x <span class=\"token operator\">=</span> layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Define the Encoder Layer</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">EncoderLayer</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>EncoderLayer<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>self_attention <span class=\"token operator\">=</span> MultiHeadAttention<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>feedforward <span class=\"token operator\">=</span> FeedForward<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        att_output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>self_attention<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> att_output <span class=\"token operator\">+</span> x\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>feedforward<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\"># Define the Multi-Head Attention mechanism</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">MultiHeadAttention</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>MultiHeadAttention<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>num_heads <span class=\"token operator\">=</span> num_heads\n        self<span class=\"token punctuation\">.</span>head_size <span class=\"token operator\">=</span> hidden_size <span class=\"token operator\">//</span> num_heads\n        self<span class=\"token punctuation\">.</span>q_linear <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>k_linear <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>v_linear <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>out_linear <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> query<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        batch_size <span class=\"token operator\">=</span> query<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n        Q <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>q_linear<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>num_heads<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">)</span>\n        K <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>k_linear<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>num_heads<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">)</span>\n        V <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>v_linear<span class=\"token punctuation\">(</span>value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>num_heads<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">)</span>\n\n        Q <span class=\"token operator\">=</span> Q<span class=\"token punctuation\">.</span>permute<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n        K <span class=\"token operator\">=</span> K<span class=\"token punctuation\">.</span>permute<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n        V <span class=\"token operator\">=</span> V<span class=\"token punctuation\">.</span>permute<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n\n        att_scores <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>Q<span class=\"token punctuation\">,</span> K<span class=\"token punctuation\">.</span>permute<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> self<span class=\"token punctuation\">.</span>head_size<span class=\"token operator\">**</span><span class=\"token number\">0.5</span>\n        att_scores <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>att_scores<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n        att_output <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>att_scores<span class=\"token punctuation\">,</span> V<span class=\"token punctuation\">)</span>\n        att_output <span class=\"token operator\">=</span> att_output<span class=\"token punctuation\">.</span>permute<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>contiguous<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>num_heads <span class=\"token operator\">*</span> self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">)</span>\n\n        att_output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>out_linear<span class=\"token punctuation\">(</span>att_output<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> att_output\n\n<span class=\"token comment\"># Define the Feed-Forward Layer</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">FeedForward</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>FeedForward<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span> <span class=\"token operator\">*</span> hidden_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">4</span> <span class=\"token operator\">*</span> hidden_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\"># Initialize the model and define a simple training loop</span>\ninput_size <span class=\"token operator\">=</span> <span class=\"token number\">2</span>  <span class=\"token comment\"># 0 and 1 in the sequence</span>\nhidden_size <span class=\"token operator\">=</span> <span class=\"token number\">256</span>\nnum_heads <span class=\"token operator\">=</span> <span class=\"token number\">4</span>\nnum_layers <span class=\"token operator\">=</span> <span class=\"token number\">2</span>\nseq_length <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n\nmodel <span class=\"token operator\">=</span> Transformer<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\ncriterion <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Generate a simple input sequence (1 0 1 0 1 0...)</span>\ninput_seq <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>seq_length <span class=\"token operator\">//</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">)</span>\ntarget_seq <span class=\"token operator\">=</span> input_seq<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>  <span class=\"token comment\"># Shift the target by one time step</span>\n\n<span class=\"token comment\"># Training loop</span>\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_seq<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> target_seq<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> target_seq<span class=\"token punctuation\">)</span>\n    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>epoch <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> <span class=\"token number\">100</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Epoch [</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>epoch <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">/1000], Loss: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Test the model</span>\n<span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    test_input <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>seq_length <span class=\"token operator\">//</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">)</span>\n    predicted_output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>test_input<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> test_input<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>seq_length <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    predicted_sequence <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>predicted_output<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Input Sequence:\"</span><span class=\"token punctuation\">,</span> test_input<span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Predicted Sequence:\"</span><span class=\"token punctuation\">,</span> predicted_sequence<span class=\"token punctuation\">)</span>\n</code></pre></div>\n<h2 id=\"homework\" style=\"position:relative;\">Homework<a href=\"#homework\" aria-label=\"homework permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<ul>\n<li>What is a usual window size for attention in current models? See <a href=\"https://stats.stackexchange.com/questions/411736/why-do-attention-models-need-to-choose-a-maximum-sentence-length/411919#411919\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">here</a> why it's theoretically infinite</li>\n<li>How Transformers solve problem with <a href=\"https://smerity.com/articles/2017/mixture_of_softmaxes.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">overfitting usually caused by softmax</a>?</li>\n</ul>\n<blockquote>\n<p>The softmax allows you to produce a probability distribution over a set of classes</p>\n</blockquote>","excerpt":"Questions Are embeddings required for any type of data? Why sin/cos? What stacks add to the whole picture? How are they different? Why…","tableOfContents":"<ul>\n<li><a href=\"#questions\">Questions</a></li>\n<li><a href=\"#reference\">Reference</a></li>\n<li><a href=\"#input-data-and-goal\">Input data and goal</a></li>\n<li><a href=\"#code\">Code</a></li>\n<li><a href=\"#homework\">Homework</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/bf0eddd7-c885-5815-8d65-8ba8c9bf0651.jpg"},"frontmatter":{"date":"June 03, 2023","published":"June 03, 2023","lastModified":"May 26, 2024","title":"The smallest Transformer","subtitle":null,"section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/the-smallest-transformer.md","url":"/ai/the-smallest-transformer","next":{"excerpt":"Initial plan read about crop blur technique in modern datasets. (I think I’ve seen it when I read about Celeb HQ)  crop to faces: recognize…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/my-dataset.md","frontmatter":{"title":"My Small Training Dataset","date":"2023-07-17T00:00:00.000Z","topic":null,"article":"quest"},"id":"fe68679e-fb93-5434-b9d2-b7bd9f568fdf"},"previous":{"excerpt":"Also: Stable Diffusion and 3D Tutorials https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ Another diffusion tutorial https…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/diffusion-models.md","frontmatter":{"title":"Diffusion models","date":"2023-05-27T00:00:00.000Z","topic":true,"article":null},"id":"f5fdb83b-084d-5996-b6c6-a012d6327328"},"recentArticles":[{"excerpt":"BAM can link together data of different types. Associations...  From one side the model requires to use bipolar patterns - arrays of -1 and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/bidirectional-associative-memories.md","frontmatter":{"title":"Bidirectional Associative Memories","date":null,"topic":true,"article":null},"id":"3ae9ac87-4c8b-5f18-8f19-ee1091111f95"},{"excerpt":"Phenotypic variation of transcriptomic cell types in mouse motor cortex (paper). Scientist got neurons from different layers and regions and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/brain-map.md","frontmatter":{"title":"Brain map","date":null,"topic":true,"article":null},"id":"f4152a9b-6747-5178-aea3-0de2e3f70b04"},{"excerpt":"Neural networks with biologically plausible accounts of neurogenesis they start with a minimal topology (just input and output unit) and…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cascade-correlation-neural-networks.md","frontmatter":{"title":"Cascade Correlation Neural Networks","date":null,"topic":true,"article":null},"id":"1ba88be8-cdba-50c0-a9ba-c4a082ef60e7"},{"excerpt":"ACT-R CLARION LIDA Soar (code)","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/cognitive-architecture.md","frontmatter":{"title":"Cognitive Architecture","date":null,"topic":true,"article":null},"id":"1acc6cfc-d238-53b6-aeed-1207dd3b1d89"},{"excerpt":"Main paper PDF stacking lstms https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms intro to…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/ai/long-short-term-memory.md","frontmatter":{"title":"Long Short-Term Memory","date":null,"topic":true,"article":null},"id":"a677bfb4-f49d-559c-8957-86db6eb41bda"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}