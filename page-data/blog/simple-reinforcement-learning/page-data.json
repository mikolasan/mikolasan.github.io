{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/simple-reinforcement-learning","result":{"data":{"markdownRemark":{"html":"<h2 id=\"state-representation\" style=\"position:relative;\">State representation<a href=\"#state-representation\" aria-label=\"state representation permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Make sure that graphical representation matches with internal data which usually means values in tensors. If it's a 2D world then check that axis are not swapped.\nThis is where I found that an agent advances after the game is over</p>\n<h2 id=\"model\" style=\"position:relative;\">Model<a href=\"#model\" aria-label=\"model permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Verify that the network is learning. After every training step the outcome should be slightly (or not slightly) different. Training process is not always goes the same way but networks itself are deterministic and forward output is always predictable.\nWith model freeze and deepcopy gradient can be broken in PyTorch. Check that <code class=\"language-text\">print(list(self.model.parameters())[0].grad)</code> is not <code class=\"language-text\">None</code>.</p>\n<h2 id=\"replay-buffer\" style=\"position:relative;\">Replay buffer<a href=\"#replay-buffer\" aria-label=\"replay buffer permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Should the final reward be included?</p>\n<h2 id=\"batches\" style=\"position:relative;\">Batches<a href=\"#batches\" aria-label=\"batches permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>PyTorch allows to feed batched input to the model but you need to be careful what axis you use for stacking matrices. As I understand it must be the first one. Use <a href=\"https://pytorch.org/docs/stable/generated/torch.stack.html\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><code class=\"language-text\">torch.stack</code></a> for that. I've seen <a href=\"https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\"><code class=\"language-text\">torch.cat</code></a> used instead but I assume that your original data then must be of another configuration. 2D arrays must be flattened before processed through the Linear module.\nYour model size doesn't need to be enlarged by batch size.</p>\n<h2 id=\"problem-of-popular-courses\" style=\"position:relative;\">Problem of popular courses<a href=\"#problem-of-popular-courses\" aria-label=\"problem of popular courses permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>As I know the entry course from the books, it’s a dead end. There are several problems. First, it’s the state from where you take action. It only includes one cell where you are standing. I added one cell around and it exponentially increased complexity of the following training by gradient descent. Which means deep q learning is not a magical solution.\nSo instead of 1^4 it’s (1+4)^4.\nSecond thing is memory. It’s applied for Atari games, but it’s really important. Also anticipation. Using past observations and pattern extraction, the agent will be predicting probabilities of actions step ahead. So, a course of previous and future actions is always in processing.\nBut how more steps can be the in transition matrix without blowing up the size?\nAnd emotions. Once it learns that a hole is leading to a negative reward it can say that about any new observation where the hole is present (especially for moving into that direction)</p>\n<h2 id=\"continuous-variables-in-state-space\" style=\"position:relative;\">Continuous variables in state space<a href=\"#continuous-variables-in-state-space\" aria-label=\"continuous variables in state space permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Discrete MDP and model minimization ( Dean Givan)</p>\n<p>And one approach would be to find similar situations and treat them as one. Where values sort of going through discretization process but not based on ready set intervals but more like grouped by results.</p>\n<p>Also here we can try to apply the most powerful technique that you can find in any field reappearing over and over. It’s adding hierarchy. It folds so many possibilities in a little space and saves many outcomes and situations behind little triggers.\nSo every state is a combination of such hierarchical states in their first hierarchy level. But next level doesn’t change the original state though it has its own transition matrix that has an effect on the action.</p>\n<h2 id=\"next\" style=\"position:relative;\">Next<a href=\"#next\" aria-label=\"next permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>Then you realize that one reward function is not enough. You define several and set priority rules between them. It will work as conditions to prefer one goal over another.</p>\n<p>Find an article about a beaver escaping from prison</p>\n<p>To test network correctness we need simpler environments. First, check that rewards work. There are two actions, red and blue, one is positive and another is negative. As a result only blue option (positive reward) must be chosen.\nNext, three or more options and different reward values, like a range in a scale. Check distribution of choosing every option.\nThen walking. State is specific here /<need to think/></p>\n<p>I believe a simple neural network cannot learn the notion of distance from a set of inputs representing absolute positions and reward values awarded when the distance becomes zero.</p>\n<p>I already was reviewing this question. I don’t remember for sure, maybe I have read it, but humans can only approximate distances that they observe. Or maybe that was about time? Anyway, one can measure and feel with one’s body but vision is not giving the answer. And if we look at the snake game as an observer (like if we are not the snake, and her head is not our point of view) then there is no way we can precisely calculate where the apple is.\nSo then we just notice relative positions, relations between objects.\n<a href=\"https://mastodon.social/@mikolasan/112794749604350118\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://mastodon.social/@mikolasan/112794749604350118</a>\n<a href=\"https://arxiv.org/abs/2407.06723\" target=\"_blank\" rel=\"external nofollow noopener noreferrer\">https://arxiv.org/abs/2407.06723</a>\n“Humans describe complex scenes with compositionality, using simple text descriptions enriched with links and relationships. […] this is not reflected yet in existing datasets which, for the most part, still use plain text to describe images. In this work, we propose a new annotation strategy, graph-based captioning that describes an image using a labelled graph structure”\nBut that description is obtained by open source lava model. Though encoding positions as words is interesting.</p>\n<h2 id=\"negative-events\" style=\"position:relative;\">Negative events<a href=\"#negative-events\" aria-label=\"negative events permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>While playing with the snake environment, I noticed that negative outcomes don’t really teach an agent to avoid actions or previous actions leading to such results. And more often it learns something but not in a way rewards were assigned. For example if on every step when the snake is not hitting a wall the total reward goes up  (we reward longer episodes without hitting a wall) then spinning in circles on one place will accumulate enough to exceed one big negative reward of hitting the wall. And if we give a small negative reward on every step to stimulate the agent to pick up fruits with higher reward more often then the agent can find out that hitting the wall on early stages is more optimal than searching for apples.</p>\n<h2 id=\"parents\" style=\"position:relative;\">Parents<a href=\"#parents\" aria-label=\"parents permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>One agent can learn from actions of another. That reward comes in a multitude of regular training and exploration</p>\n<h2 id=\"vityaev\" style=\"position:relative;\">Vityaev<a href=\"#vityaev\" aria-label=\"vityaev permalink\" class=\"with-anchor after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\">\n                <path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3\n                3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3\n                  9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64\n                  1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\">\n                </path>\n                </svg></a></h2>\n<p>What if instead of long tables of data where the amount of columns exponentially increases exhaustive search instead the rules will be obtained and their probabilities improved by feeding data line by line.\nAlso RF usually operates on matrices and Markov processes but what about “rules” - bayeasian causality with probabilities</p>","excerpt":"State representation Make sure that graphical representation matches with internal data which usually means values in tensors. If it's a 2D…","tableOfContents":"<ul>\n<li><a href=\"#state-representation\">State representation</a></li>\n<li><a href=\"#model\">Model</a></li>\n<li><a href=\"#replay-buffer\">Replay buffer</a></li>\n<li><a href=\"#batches\">Batches</a></li>\n<li><a href=\"#problem-of-popular-courses\">Problem of popular courses</a></li>\n<li><a href=\"#continuous-variables-in-state-space\">Continuous variables in state space</a></li>\n<li><a href=\"#next\">Next</a></li>\n<li><a href=\"#negative-events\">Negative events</a></li>\n<li><a href=\"#parents\">Parents</a></li>\n<li><a href=\"#vityaev\">Vityaev</a></li>\n</ul>","fields":{"socialcard":"gatsby-plugin-social-card/baabef51-5f03-544c-b6bc-edce76feba2f.jpg"},"frontmatter":{"date":"August 05, 2024","published":"August 05, 2024","lastModified":"August 05, 2024","title":"Simple Reinforcement Learning","subtitle":"Looking for mistakes in Snake environment","section":null,"draft":null,"developing":null,"buttonText":null,"buttonLink":null,"secondButtonText":null,"secondButtonLink":null,"featuredImage":null}}},"pageContext":{"showLikes":true,"absolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/simple-reinforcement-learning.md","url":"/blog/simple-reinforcement-learning","next":{"excerpt":"I decided to create this page because I often Anaconda cheatsheet My note on how to install Anaconda in VSCode terminal Common tasks There…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/all-about-conda.md","frontmatter":{"title":"All about Conda","date":"2024-08-05T00:00:00.000Z","topic":null,"article":null},"id":"ef4ebdff-89f1-5f2e-ab58-1f06fe19ebc9"},"previous":{"excerpt":"I envy my brother. When I was very little, he was finishing school and I remember his homework. It has been moments that you carry in your…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/stars-and-polyhedron.md","frontmatter":{"title":"Stars and polyhedron","date":"2024-07-12T00:00:00.000Z","topic":null,"article":null},"id":"9b0b0c07-4e7b-5357-8dfc-310b072377b0"},"recentArticles":[{"excerpt":"To read AI hype is over? (post from 2023) - L’IA n’est-elle qu’un mythe qui s’essoufflera ? About Google alternative services -  Comment j…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/learning-french.md","frontmatter":{"title":"How I am learning French","date":"2025-07-12T00:00:00.000Z","topic":null,"article":null},"id":"de46ffde-c202-594f-b222-f5f4a548fbe4"},{"excerpt":"I think I like having a dialogue with someone who knows about the topic, even with students who just obtained knowledge from canonical…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/lizards-and-continuous-space.md","frontmatter":{"title":"Lizards and Continuous space","date":"2025-06-23T00:00:00.000Z","topic":null,"article":null},"id":"6ee1f95a-2c05-5950-8a0d-20ff4d937fb1"},{"excerpt":"Yes, developers add new features, refactor other parts to get rid of technical debt. This often changes API, which usually triggers…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/new-ui-and-conspiracy-in-it-industry.md","frontmatter":{"title":"New UI","date":"2025-06-21T00:00:00.000Z","topic":null,"article":null},"id":"fe2e710c-0afa-5b11-8a7d-1dc46d02f0f9"},{"excerpt":"Now when I almost finished with the titanic competition (I didn’t reach 90% accuracy as I planned, although I doubt it’s possible), it’s…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/regulus.md","frontmatter":{"title":"Regulus","date":"2025-05-13T00:00:00.000Z","topic":null,"article":null},"id":"cf23a6fc-e0e2-571b-9dbf-5030b3af25dd"},{"excerpt":"When I found out there was a popular platform where people share open source projects, I imagined GitHub differently. I thought I could find…","fileAbsolutePath":"/home/runner/work/mikolasan.github.io/mikolasan.github.io/src/markdown/blog/how-github-should-work.md","frontmatter":{"title":"How GitHub should work","date":"2025-04-03T00:00:00.000Z","topic":null,"article":null},"id":"63562cc8-f132-5cbc-a444-5639c4a78d98"}]}},"staticQueryHashes":["2961657013","447685113"],"slicesMap":{}}