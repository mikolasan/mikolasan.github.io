---
path: "/ru/science/memes"
date: "2020-08-11"
title: Мемы
language: "ru"
draft: true
---

[Соревнование](https://twitter.com/stakanmartini/status/1283281648360886272) по классификации мемов, которое на первый взгляд кажется полностью подходит под [обработку естественного языка](/ru/science/nlp-introduction).


## Критерий

A direct or indirect attack on people based on characteristics, including ethnicity, race, nationality, immigration status, religion, caste, sex, gender identity, sexual orientation, and disability or disease. We define attack as violent or dehumanizing (comparing people to non-human things, e.g. animals) speech, statements of inferiority, and calls for exclusion or segregation. Mocking hate crime is also considered hate speech.


## План

На картинках есть важная информация, которую нужно извлечь. Эта информация не заключена в пикселях, она заключена в контексте, который нужно понимать (извлекать, майнить). Если бы важная информация заключалась в самих картинках, то достаточно было бы предоставленого датасета. Но в поставленной задаче **картинки никак между собой не связаны**, они только лишь несут дополнительную информацию (контекст) к написанному тексту.

Значит ли это что нам нужно расширить датасет? На какие категории нужно нацелить предварительное обучение? Например, по картинки, если на ней изображен человек, определять пол, расу, возраст - вещи по которым можно провести дискриминацию. Сколько процентов из всего датасета будет покрыто при одном только [распозновании лиц](/ru/science/face-recognition)?


## Текст

Первая идея заключается в том, чтобы использовать классификацию из обучения и выделить из предложений слова, которые можно назвать маркером ненависти. Если эти слова будут встречаться на тестах, то такие предложения будут соответственно классицироваться. Если слова не повторяются, то такая модель не работает.

Можно ли находить закономерности и применять их по подобию к другим словам?