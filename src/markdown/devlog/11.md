---
date: 2023-07-11
---

Sensor has a receptive field and some dendrites that come from the higher neurons back. When something appears in the receptive field, the neuron starts firing with a high rate. The signal goes through some modifications when it crosses layers above and above. Then some blocks try to find it in memory. And if no plasticity needed, then the inhibitory signal comes back, Back to the sensor and together with this slow down command the rate slows down. And signal is not going to the memory. No extra plasticity activated over and overÂ 



Hebbian learning and gradient descent are two distinct mechanisms that are used to describe learning and adaptation in neural networks, but they operate at different levels and have different purposes. Here's an overview of the differences between Hebbian learning and gradient descent:

1. Level of Operation:
    - Hebbian Learning: Hebbian learning is a cellular-level learning rule that describes how the connections between individual neurons can be modified based on their activity. It focuses on the changes in synaptic strength between neurons.
    - Gradient Descent: Gradient descent operates at a higher level, typically used in the context of training artificial neural networks. It optimizes the network's parameters, such as weights and biases, to minimize an error or loss function.
2. Purpose:
    - Hebbian Learning: Hebbian learning is a biologically inspired rule that suggests that synaptic connections between neurons are strengthened when those neurons are activated simultaneously. It is often associated with the formation and refinement of neural circuits and the establishment of associative learning.
    - Gradient Descent: Gradient descent is an optimization algorithm used to train neural networks by iteratively adjusting the parameters to minimize a specific cost or loss function. It is employed to find the optimal set of parameters that result in the best network performance.
3. Mechanism:
    - Hebbian Learning: In Hebbian learning, synaptic connections are modified based on the correlation between pre-synaptic and post-synaptic activity. If the pre-synaptic neuron's activity consistently precedes the firing of the post-synaptic neuron, the connection between them is strengthened.
    - Gradient Descent: Gradient descent uses the gradient of the cost function with respect to the network's parameters to iteratively update the parameters in a direction that minimizes the cost. It calculates the gradients by backpropagating the error through the network and adjusts the parameters accordingly.
4. Application Domain:
    - Hebbian Learning: Hebbian learning has been proposed as a mechanism for various forms of synaptic plasticity and learning in biological neural networks. It is associated with processes such as long-term potentiation (LTP) and long-term depression (LTD), which are believed to be involved in memory formation and synaptic modification.
    - Gradient Descent: Gradient descent is widely used in training artificial neural networks for various tasks such as image classification, natural language processing, and reinforcement learning. It allows the network to learn from training data and optimize its parameters to improve performance.

In summary, Hebbian learning and gradient descent operate at different levels and serve different purposes. Hebbian learning describes synaptic plasticity and cellular-level modifications in neural connections, while gradient descent is an optimization algorithm used to train artificial neural networks by adjusting their parameters to minimize a cost or loss function.