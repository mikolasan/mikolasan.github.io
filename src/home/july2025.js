export const articles = [
  {
    style: `box1`,
    title: `Regulus`,
    subtitle: ``,
    url: `/blog/regulus`,
    excerpt: `<p>Rules are like descriptions of spiking activity. 
    A premise holds pre-synaptic neurons. 
    And when receptors receive signals these rules describe what next neurons will be excited or inhibited</p>`,
    readMore: ``,
    imgSrc: ``,
    imgAlt: ``,
  },
  {
    style: `box2`,
    title: `Uncomfortable Robots`,
    subtitle: `How do we train robots to feel “comfortable” in some states and not others?`,
    url: `/blog/uncompfortable-robots`,
    excerpt: `<p>One could define internal goals and value structures, 
    and train it on goal-aligned tasks where some paths feel cheap and good, others costly and conflictual. 
    Discomfort would arise when there's mismatch with internal model and increase in prediction error. 
    But if robot just optimizes reward, it might do things that look wrong to us.</p>`,
    readMore: ``,
    imgSrc: ``,
    imgAlt: ``,
  },
  {
    style: `box3`,
    title: `Intelligent work with pixels`,
    subtitle: ``,
    url: `/blog/intelligent-work-with-pixels`,
    excerpt: ``,
    readMore: ``,
    imgSrc: ``,
    imgAlt: ``,
  },
  {
    style: `box4`,
    title: `Clean HTML`,
    subtitle: ``,
    url: `/ideas/clean-html`,
    excerpt: ``,
    readMore: ``,
    imgSrc: ``,
    imgAlt: ``,
  },
  {
    style: `box5`,
    title: `How GitHub should work`,
    subtitle: ``,
    url: `/blog/how-github-should-work`,
    excerpt: ``,
    readMore: ``,
    imgSrc: null,
    imgAlt: null,
  },
  {
    style: `box6`,
    title: `Spiking autoencoder with embodied actions`,
    subtitle: ``,
    url: `/ai/spiking-autoencoder-with-embodied-actions.md`,
    excerpt: `<p>I'm adding neurons responsible for image transformation.
    So these transformation neurons will be sending a signal to an external source that would actually change the input. 
    And when prediction is far from the observed image, that is when transformation neurons try to adjust the image.</p>`,
    readMore: ``,
    imgSrc: null,
    imgAlt: null,
  },
  {
    style: `box7`,
    title: `Task management`,
    subtitle: `with taskwarrior`,
    url: `/linux/task-management-with-taskwarrior`,
    excerpt: `<p>So I guess I’m looking for a simple personal task management app that just needs due dates and reminders. I decided to give Task Warrior a chance. It uses cmake build system—everything should be smooth, right? I tried MSVC and MSYS compilers.</p>`,
    readMore: ``,
    imgSrc: null,
    imgAlt: null,
  },
  {
    style: `box8`,
    title: `Transient RNN`,
    subtitle: ``,
    url: `/ai/reviews/workink-memory-encoding-mechanism-explained`,
    excerpt: `<p>I was looking for some insights about encoding methods and known tricks like spatial distribution in sparse encoding while I was playing with hippocampus model.
    What is stored in memory? Short little facts or chains of events? 
    The main question is what data we need to train such memory and how this stored information is used?</p>`,
    readMore: ``,
    imgSrc: null,
    imgAlt: null,
  },
  {
    style: `box9`,
    title: `Keep render and logic separate`,
    subtitle: ``,
    url: `/code/cpp/render-and-logic`,
    excerpt: `<p>I have nodes and a graph class that defines connections between nodes, 
    and I think there’s a choice here—a render class that includes logic (like in any game engine) or
    the logic includes rendering functions (which makes rendering optional)..</p>`,
    readMore: ``,
    imgSrc: null,
    imgAlt: null,
  },
];